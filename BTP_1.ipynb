{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJHH62nCLYkmL5hZWpde1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meet-13/image-Inpainting/blob/main/BTP_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "bQNgoOxI5auz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bias_act\n"
      ],
      "metadata": {
        "id": "YRsRKO-T9k2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fenglinglwb/MAT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HaV5rYLD8z5",
        "outputId": "c5d2ca2c-73f4-4068-dd47-a06ca82fc490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MAT'...\n",
            "remote: Enumerating objects: 199, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 199 (delta 42), reused 39 (delta 39), pack-reused 148 (from 1)\u001b[K\n",
            "Receiving objects: 100% (199/199), 18.89 MiB | 15.67 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/MAT')\n",
        "from torch_utils.ops import upfirdn2d\n",
        "from torch_utils.ops import bias_act\n"
      ],
      "metadata": {
        "id": "zissFdW4Ewat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully Connected Layer\n"
      ],
      "metadata": {
        "id": "4izQP0Il49sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Define Fully Connected Layer with hardcoded bias_act function\n",
        "class FullyConnectedLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,                # Number of input features.\n",
        "                 out_features,               # Number of output features.\n",
        "                 bias=True,                  # Apply additive bias before the activation function?\n",
        "                 activation='linear',        # Activation function: 'relu', 'lrelu', etc.\n",
        "                 lr_multiplier=1,            # Learning rate multiplier.\n",
        "                 bias_init=0):               # Initial value for the additive bias.\n",
        "        super().__init__()\n",
        "        # Initialize weights and biases\n",
        "        self.weight = torch.nn.Parameter(torch.randn([out_features, in_features]) / lr_multiplier)\n",
        "        self.bias = torch.nn.Parameter(torch.full([out_features], np.float32(bias_init))) if bias else None\n",
        "        self.activation = activation\n",
        "\n",
        "        # Gain values for weight scaling\n",
        "        self.weight_gain = lr_multiplier / np.sqrt(in_features)\n",
        "        self.bias_gain = lr_multiplier\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute scaled weight\n",
        "        w = self.weight * self.weight_gain\n",
        "        b = self.bias\n",
        "        if b is not None and self.bias_gain != 1:\n",
        "            b = b * self.bias_gain\n",
        "\n",
        "        # Matrix multiplication\n",
        "        x = x.matmul(w.t())\n",
        "\n",
        "        # Bias addition\n",
        "        if b is not None:\n",
        "            x = x + b.reshape([-1 if i == x.ndim-1 else 1 for i in range(x.ndim)])\n",
        "\n",
        "        # Apply activation\n",
        "        x = bias_act.bias_act(x, act=self.activation)\n",
        "        return x"
      ],
      "metadata": {
        "id": "P3nt0ESK5HvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "# Define input tensor\n",
        "input_tensor = torch.randn(4, 128)  # Example input\n",
        "\n",
        "# Initialize Fully Connected Layer\n",
        "fc_layer = FullyConnectedLayer(in_features=128, out_features=64, activation='relu')\n",
        "\n",
        "# Apply layer to input tensor\n",
        "output = fc_layer(input_tensor)\n",
        "\n",
        "# Print output\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R8qjMRD8juQ",
        "outputId": "2b3ce80b-827f-442a-c95f-6f4b3ff19794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.7422, 0.0000, 0.0000, 1.6625, 1.9158, 0.1447, 0.1280, 0.0000, 1.5578,\n",
            "         0.5332, 0.8277, 0.0000, 0.0000, 0.0000, 1.6467, 0.2258, 0.0000, 1.1077,\n",
            "         0.0000, 0.0000, 1.2268, 1.9813, 0.0000, 0.6802, 3.4428, 0.8143, 0.0000,\n",
            "         0.0000, 0.4108, 0.0000, 0.0000, 1.0681, 0.7369, 0.0000, 2.2316, 2.1901,\n",
            "         0.8963, 0.0000, 0.8565, 0.7958, 0.9232, 2.0932, 0.0256, 0.5827, 1.9466,\n",
            "         0.9894, 2.2214, 0.4001, 0.0000, 0.0586, 0.2943, 0.0000, 2.4943, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.2094, 2.3620, 0.0000, 0.0000, 1.5645, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4738, 0.5696, 0.0714, 0.6723, 0.2987,\n",
            "         1.6035, 0.0000, 0.0000, 0.0000, 0.5221, 0.0000, 0.0000, 1.5053, 0.0000,\n",
            "         1.7508, 0.0000, 0.1932, 0.0000, 2.3073, 0.0000, 0.3389, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 2.3375, 1.2715, 0.4967, 0.0000, 1.5017, 0.9657,\n",
            "         0.0000, 0.3186, 0.0000, 0.0000, 1.3415, 1.8052, 1.7870, 0.1630, 0.0000,\n",
            "         0.0579, 0.1405, 0.0000, 1.2492, 0.0000, 1.1415, 0.0000, 1.2148, 3.6847,\n",
            "         0.0000, 2.1749, 0.0000, 0.0000, 0.2150, 0.0000, 0.0000, 0.5131, 2.1743,\n",
            "         0.6044],\n",
            "        [0.7519, 1.3433, 0.0000, 1.3249, 0.0000, 0.0000, 0.0000, 2.2052, 1.0305,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.4201, 0.0000, 0.1859, 0.0000, 2.1763,\n",
            "         0.2421, 0.0000, 0.5415, 1.3391, 1.1327, 0.8291, 2.0273, 1.5949, 4.0065,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.3984, 0.1293, 1.0339, 0.6070,\n",
            "         1.3889, 1.5942, 0.3737, 2.2650, 0.0000, 1.1294, 0.0000, 0.0000, 0.4200,\n",
            "         0.0000, 0.0000, 0.0000, 2.8881, 0.0000, 0.1350, 0.0000, 0.0000, 0.0000,\n",
            "         0.2031, 0.0000, 0.2638, 0.0000, 0.0185, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000],\n",
            "        [0.0000, 2.3751, 0.0000, 1.2267, 0.0000, 0.0000, 1.2265, 2.5486, 0.0000,\n",
            "         1.3643, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.7973, 1.7343, 0.0000,\n",
            "         2.3210, 1.0478, 1.0435, 0.0000, 0.1462, 0.0000, 0.0000, 0.1805, 0.0000,\n",
            "         0.0000, 1.8496, 2.3510, 1.2378, 0.4073, 0.9523, 1.7681, 0.0000, 0.0000,\n",
            "         0.0000, 1.3992, 0.0474, 0.0000, 0.6304, 0.0000, 0.1775, 0.7774, 0.0000,\n",
            "         0.0000, 0.0000, 3.1902, 1.7042, 4.0843, 1.1987, 2.3176, 0.0000, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 2.2617, 0.0897,\n",
            "         0.6224]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv2dlayer\n",
        "\n",
        "The Conv2dLayer class is designed as a custom convolutional layer in a neural network, similar to what you would find in popular architectures like convolutional neural networks (CNNs)"
      ],
      "metadata": {
        "id": "ZKOEiHWQ-CCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Conv2dLayer\n",
        "class Conv2dLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,                    # Number of input channels.\n",
        "                 out_channels,                   # Number of output channels.\n",
        "                 kernel_size,                    # Width and height of the convolution kernel.\n",
        "                 bias=True,                      # Apply additive bias before the activation function?\n",
        "                 activation='linear',            # Activation function: 'relu', 'lrelu', etc.\n",
        "                 up=1,                           # Integer upsampling factor.\n",
        "                 down=1,                         # Integer downsampling factor.\n",
        "                 resample_filter=[1, 3, 3, 1],   # Low-pass filter to apply when resampling activations.\n",
        "                 conv_clamp=None,                # Clamp the output to +-X, None = disable clamping.\n",
        "                 trainable=True):                # Update the weights of this layer during training?\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.padding = kernel_size // 2\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        weight = torch.randn([out_channels, in_channels, kernel_size, kernel_size])\n",
        "        bias = torch.zeros([out_channels]) if bias else None\n",
        "\n",
        "        if trainable:\n",
        "            self.weight = torch.nn.Parameter(weight)\n",
        "            self.bias = torch.nn.Parameter(bias) if bias is not None else None\n",
        "        else:\n",
        "            self.register_buffer('weight', weight)\n",
        "            if bias is not None:\n",
        "                self.register_buffer('bias', bias)\n",
        "            else:\n",
        "                self.bias = None\n",
        "\n",
        "    def forward(self, x, gain=1):\n",
        "        # Apply convolution\n",
        "        w = self.weight * self.weight_gain\n",
        "        x = F.conv2d(x, w, padding=self.padding)\n",
        "\n",
        "        # Apply activation, gain, and clamping using bias_act function\n",
        "        act_gain = gain\n",
        "        act_clamp = self.conv_clamp * gain if self.conv_clamp is not None else None\n",
        "        x = bias_act.bias_act(x, self.bias, act=self.activation, gain=act_gain, clamp=act_clamp)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "gnOYiv03FdjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "# Define input tensor\n",
        "input_tensor = torch.randn(4, 3, 32, 32)  # Example input with batch size 4, 3 channels, 32x32 height/width\n",
        "\n",
        "# Initialize Conv2d Layer\n",
        "conv_layer = Conv2dLayer(in_channels=3, out_channels=16, kernel_size=3, activation='relu')\n",
        "\n",
        "# Apply layer to input tensor\n",
        "output = conv_layer(input_tensor)\n",
        "\n",
        "# Print output\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfcToWf496mG",
        "outputId": "c121e3b2-e29c-46d5-a225-99eddb694ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.0000e+00, 4.5134e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.6759e-01,  ..., 1.2356e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [4.9374e-01, 2.2984e-01, 3.7355e-01,  ..., 0.0000e+00,\n",
            "           7.1948e-01, 6.9212e-01],\n",
            "          ...,\n",
            "          [5.8218e-01, 5.9770e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 5.9609e-01],\n",
            "          [6.7606e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.6319e-01, 3.2272e-01],\n",
            "          [2.7260e-01, 2.4159e-01, 3.2315e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1750e+00,\n",
            "           0.0000e+00, 4.4311e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0591e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [7.8460e-01, 0.0000e+00, 9.5420e-01,  ..., 0.0000e+00,\n",
            "           1.0078e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 2.2862e-01,  ..., 1.4348e+00,\n",
            "           4.3047e-01, 0.0000e+00],\n",
            "          [3.8729e-01, 0.0000e+00, 7.6713e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 8.6458e-01, 1.5655e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 3.0201e-01]],\n",
            "\n",
            "         [[3.5012e-01, 6.7748e-01, 1.1978e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4144e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [3.4908e-01, 0.0000e+00, 1.3470e+00,  ..., 3.6468e-01,\n",
            "           1.4430e+00, 7.2747e-01],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 1.7832e-01,  ..., 1.7196e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [9.2310e-01, 0.0000e+00, 0.0000e+00,  ..., 7.8230e-01,\n",
            "           5.0442e-01, 5.1904e-01],\n",
            "          [0.0000e+00, 5.0988e-01, 0.0000e+00,  ..., 2.5748e+00,\n",
            "           1.1006e+00, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.5264e+00, 1.3454e+00, 5.8475e-01,  ..., 5.3782e-01,\n",
            "           1.7456e+00, 1.4911e-01],\n",
            "          [5.2649e-01, 1.9969e-01, 0.0000e+00,  ..., 5.3962e-01,\n",
            "           1.1303e+00, 6.4635e-01],\n",
            "          [1.3733e+00, 0.0000e+00, 2.1879e-01,  ..., 3.4781e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [8.4894e-01, 6.1667e-01, 7.0728e-01,  ..., 0.0000e+00,\n",
            "           3.6462e-02, 9.1487e-01],\n",
            "          [6.2152e-01, 0.0000e+00, 1.9610e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [5.9867e-01, 0.0000e+00, 3.5758e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 4.2529e-01, 5.7319e-01,  ..., 6.5487e-01,\n",
            "           2.8581e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 1.4113e+00, 9.8275e-01,  ..., 0.0000e+00,\n",
            "           1.7719e-01, 1.6624e+00],\n",
            "          [5.7291e-01, 1.3022e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.1808e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 6.6399e-01,  ..., 0.0000e+00,\n",
            "           1.0242e+00, 2.2490e+00],\n",
            "          [0.0000e+00, 1.4297e+00, 1.1906e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.2439e+00],\n",
            "          [0.0000e+00, 5.4085e-01, 9.3633e-02,  ..., 0.0000e+00,\n",
            "           9.6844e-02, 1.2682e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 3.9049e-01,  ..., 6.8499e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [5.8193e-01, 1.5449e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           3.8826e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.2361e-01, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           3.9121e-01, 0.0000e+00],\n",
            "          [1.6971e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.4215e+00, 0.0000e+00, 5.6913e-01,  ..., 1.4990e-01,\n",
            "           9.5180e-01, 0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[0.0000e+00, 1.0744e+00, 0.0000e+00,  ..., 2.4498e-01,\n",
            "           1.3278e+00, 0.0000e+00],\n",
            "          [9.5784e-02, 0.0000e+00, 1.7016e+00,  ..., 1.0404e+00,\n",
            "           2.1695e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 1.2808e-01, 0.0000e+00,  ..., 1.8631e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.3479e-01,\n",
            "           9.5679e-01, 3.3297e-01],\n",
            "          [8.5536e-01, 0.0000e+00, 0.0000e+00,  ..., 3.8671e-01,\n",
            "           5.6056e-01, 0.0000e+00],\n",
            "          [1.7305e-01, 0.0000e+00, 4.7926e-01,  ..., 4.5174e-01,\n",
            "           1.3848e+00, 2.3403e-01]],\n",
            "\n",
            "         [[0.0000e+00, 1.7389e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.5231e+00, 1.5745e-01],\n",
            "          [1.6470e-01, 0.0000e+00, 7.8376e-01,  ..., 1.0957e+00,\n",
            "           0.0000e+00, 5.0486e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 7.4091e-01,  ..., 3.9502e-01,\n",
            "           1.4259e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [2.0254e-01, 0.0000e+00, 2.5435e+00,  ..., 0.0000e+00,\n",
            "           2.9968e-01, 1.1590e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 8.7104e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 6.8766e-01],\n",
            "          [0.0000e+00, 7.6973e-01, 1.5309e-01,  ..., 3.3680e-01,\n",
            "           1.4196e+00, 5.2433e-01]],\n",
            "\n",
            "         [[0.0000e+00, 2.3328e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           3.3309e-01, 0.0000e+00],\n",
            "          [1.6126e+00, 1.4719e+00, 8.8507e-01,  ..., 0.0000e+00,\n",
            "           4.2643e-01, 3.4128e-01],\n",
            "          [5.8356e-01, 2.1661e-01, 2.6688e+00,  ..., 0.0000e+00,\n",
            "           1.8305e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 3.8284e-01,  ..., 9.7975e-03,\n",
            "           0.0000e+00, 1.1817e+00],\n",
            "          [0.0000e+00, 1.1391e+00, 8.3191e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.1580e+00],\n",
            "          [7.6999e-01, 4.7119e-02, 0.0000e+00,  ..., 1.2682e-01,\n",
            "           2.7181e-01, 6.9059e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0000e+00, 1.3053e+00, 0.0000e+00,  ..., 1.3443e-03,\n",
            "           0.0000e+00, 7.0853e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.4966e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.4125e+00, 1.2984e+00],\n",
            "          ...,\n",
            "          [1.6295e-01, 0.0000e+00, 1.6162e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.7358e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.0173e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.0335e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           4.3218e-01, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 2.9282e-01, 1.4520e-02,  ..., 1.0235e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 2.5852e-02,  ..., 2.5837e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 9.2703e-01,  ..., 1.1106e+00,\n",
            "           0.0000e+00, 1.2366e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 1.1660e+00, 0.0000e+00,  ..., 1.5280e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 3.1432e-01, 0.0000e+00,  ..., 5.8904e-01,\n",
            "           2.8113e-01, 0.0000e+00],\n",
            "          [4.1260e-01, 9.1623e-01, 5.8373e-01,  ..., 2.1873e+00,\n",
            "           1.9418e-01, 0.0000e+00]],\n",
            "\n",
            "         [[6.7149e-01, 2.1776e-01, 1.9958e+00,  ..., 2.3864e-01,\n",
            "           3.1391e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 1.8005e+00, 0.0000e+00,  ..., 9.3503e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.6174e-01, 5.7469e-01, 5.3512e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.0133e+00],\n",
            "          ...,\n",
            "          [1.3425e+00, 2.9647e-01, 0.0000e+00,  ..., 3.4761e-01,\n",
            "           2.9876e-01, 2.0226e-02],\n",
            "          [7.1128e-01, 0.0000e+00, 0.0000e+00,  ..., 9.9392e-01,\n",
            "           1.1460e+00, 9.8927e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 3.4891e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 6.9796e-01]]],\n",
            "\n",
            "\n",
            "        [[[1.3333e-01, 0.0000e+00, 0.0000e+00,  ..., 6.7426e-01,\n",
            "           4.8478e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 3.8463e-01, 4.0025e-01,  ..., 3.0445e-01,\n",
            "           0.0000e+00, 3.9967e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 3.7476e-01],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 1.2745e-02,  ..., 8.3902e-01,\n",
            "           1.0156e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.5556e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [7.5240e-01, 1.9864e-01, 0.0000e+00,  ..., 6.7379e-01,\n",
            "           0.0000e+00, 1.2351e+00]],\n",
            "\n",
            "         [[1.0335e-01, 8.2453e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           7.3572e-01, 7.2099e-01],\n",
            "          [3.2066e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.9954e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.1826e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 7.5869e-02],\n",
            "          ...,\n",
            "          [3.9169e-02, 2.6981e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 7.4187e-01],\n",
            "          [3.2550e-02, 2.4261e+00, 9.4760e-01,  ..., 0.0000e+00,\n",
            "           5.9266e-01, 3.9080e-01],\n",
            "          [6.2426e-01, 7.3562e-01, 0.0000e+00,  ..., 4.0576e-01,\n",
            "           2.5770e-01, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 2.8192e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           6.1877e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 2.3071e+00,  ..., 1.9185e-02,\n",
            "           1.0730e+00, 3.3930e-01],\n",
            "          [2.2042e-01, 1.2289e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.9062e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0952e+00,\n",
            "           5.1663e-01, 2.4880e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.8041e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [1.1448e+00, 1.1492e+00, 9.1904e-01,  ..., 1.2052e-01,\n",
            "           7.6101e-01, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.0948e+00, 1.1334e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.9385e-01, 1.8448e-01],\n",
            "          [1.9984e+00, 0.0000e+00, 4.7460e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 2.4839e-01],\n",
            "          [0.0000e+00, 1.2792e+00, 1.7332e-02,  ..., 2.3340e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 3.9586e-01, 3.3215e-02,  ..., 4.9601e-02,\n",
            "           0.0000e+00, 7.1260e-01],\n",
            "          [0.0000e+00, 4.8232e-01, 9.2704e-01,  ..., 0.0000e+00,\n",
            "           5.3044e-01, 1.5531e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.7692e-01]],\n",
            "\n",
            "         [[8.4320e-02, 7.0061e-01, 1.8751e-01,  ..., 7.5891e-01,\n",
            "           4.1678e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 9.0929e-01, 1.1105e+00,  ..., 3.6564e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 5.1934e-01, 8.6424e-01,  ..., 0.0000e+00,\n",
            "           4.9355e-01, 1.6742e-01],\n",
            "          ...,\n",
            "          [5.3388e-01, 7.4374e-01, 1.5416e+00,  ..., 0.0000e+00,\n",
            "           1.0981e+00, 2.4357e-02],\n",
            "          [5.9591e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.3253e+00, 1.5760e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.3610e-02,\n",
            "           6.7556e-01, 1.1281e-01]],\n",
            "\n",
            "         [[2.5709e-02, 1.0081e+00, 9.8108e-01,  ..., 7.3206e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [5.3982e-01, 0.0000e+00, 1.0595e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 7.5566e-02],\n",
            "          [0.0000e+00, 1.5454e-01, 2.1377e-01,  ..., 1.5495e-01,\n",
            "           5.7394e-02, 4.3082e-01],\n",
            "          ...,\n",
            "          [0.0000e+00, 4.8422e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [5.7895e-01, 4.1430e-01, 3.3105e-01,  ..., 1.2737e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 1.3912e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]]],\n",
            "\n",
            "\n",
            "        [[[1.1770e-01, 1.7146e-01, 5.1416e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 1.7950e-01],\n",
            "          [4.9764e-01, 8.2924e-01, 3.5688e-01,  ..., 0.0000e+00,\n",
            "           1.3693e-01, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1646e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 1.7725e+00, 5.9532e-01,  ..., 0.0000e+00,\n",
            "           6.5777e-01, 4.5166e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 5.1121e-01,  ..., 0.0000e+00,\n",
            "           2.3021e-01, 0.0000e+00],\n",
            "          [1.2268e+00, 8.5730e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.4369e-01, 0.0000e+00]],\n",
            "\n",
            "         [[1.5394e+00, 9.7595e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 1.3808e-01, 1.0979e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [4.9652e-01, 1.2436e+00, 1.0357e+00,  ..., 2.0189e+00,\n",
            "           0.0000e+00, 1.6647e-01],\n",
            "          ...,\n",
            "          [0.0000e+00, 8.1198e-01, 2.7067e+00,  ..., 1.8164e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [7.5793e-01, 0.0000e+00, 1.7689e-01,  ..., 8.5174e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.6400e-01,\n",
            "           8.9357e-01, 0.0000e+00]],\n",
            "\n",
            "         [[9.5314e-02, 0.0000e+00, 0.0000e+00,  ..., 7.2160e-01,\n",
            "           4.2232e-01, 1.6270e+00],\n",
            "          [1.7064e-01, 6.9552e-01, 9.8076e-01,  ..., 9.5266e-01,\n",
            "           8.7800e-01, 9.1701e-01],\n",
            "          [0.0000e+00, 1.7541e+00, 9.2121e-02,  ..., 1.0742e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [2.4327e-01, 0.0000e+00, 1.1174e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 8.7523e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.6121e-02, 1.0600e-01],\n",
            "          [5.1664e-01, 1.9801e+00, 3.4430e-01,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[1.2403e-01, 0.0000e+00, 0.0000e+00,  ..., 1.2331e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 2.7584e-01,  ..., 1.3643e+00,\n",
            "           0.0000e+00, 3.7052e-01],\n",
            "          [0.0000e+00, 6.3189e-01, 1.1862e+00,  ..., 0.0000e+00,\n",
            "           1.0574e+00, 1.0423e+00],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 4.8588e-03,  ..., 0.0000e+00,\n",
            "           1.5060e+00, 0.0000e+00],\n",
            "          [2.6867e-01, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [9.0432e-02, 0.0000e+00, 0.0000e+00,  ..., 7.5690e-01,\n",
            "           7.7314e-01, 0.0000e+00]],\n",
            "\n",
            "         [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.0300e-03, 1.2249e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           1.3706e+00, 9.7201e-01],\n",
            "          [7.3498e-02, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           2.1217e+00, 0.0000e+00],\n",
            "          ...,\n",
            "          [8.3698e-01, 0.0000e+00, 0.0000e+00,  ..., 2.1234e-02,\n",
            "           0.0000e+00, 3.2360e-01],\n",
            "          [0.0000e+00, 5.6748e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 7.7395e-01],\n",
            "          [0.0000e+00, 0.0000e+00, 7.4155e-02,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00]],\n",
            "\n",
            "         [[8.5247e-01, 1.0323e+00, 4.6750e-01,  ..., 1.7305e-01,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [6.2112e-01, 9.0950e-01, 2.9662e-01,  ..., 0.0000e+00,\n",
            "           2.8862e-01, 5.2740e-01],\n",
            "          ...,\n",
            "          [0.0000e+00, 0.0000e+00, 6.0231e-02,  ..., 0.0000e+00,\n",
            "           0.0000e+00, 0.0000e+00],\n",
            "          [8.7787e-01, 1.3861e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
            "           3.5697e-01, 0.0000e+00],\n",
            "          [1.7502e-01, 0.0000e+00, 1.1944e-01,  ..., 1.0606e+00,\n",
            "           1.0806e+00, 3.2009e-01]]]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ModulatedConv2d\n",
        "The ModulatedConv2d class is a custom convolutional layer that incorporates a modulation mechanism, typically used in advanced generative networks like StyleGAN. The modulation mechanism allows dynamic changes to the convolutional filters based on a style vector, which can help control the output style more finely"
      ],
      "metadata": {
        "id": "7egDoFV5-5Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class ModulatedConv2d(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,                   # Number of input channels.\n",
        "                 out_channels,                  # Number of output channels.\n",
        "                 kernel_size,                   # Width and height of the convolution kernel.\n",
        "                 style_dim,                     # Dimension of the style code.\n",
        "                 demodulate=True,               # Perform demodulation.\n",
        "                 up=1,                          # Integer upsampling factor.\n",
        "                 down=1,                        # Integer downsampling factor.\n",
        "                 resample_filter=[1, 3, 3, 1],  # Low-pass filter to apply when resampling activations.\n",
        "                 conv_clamp=None,               # Clamp the output to +-X, None = disable clamping.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.demodulate = demodulate\n",
        "        self.up = up\n",
        "        self.down = down\n",
        "        self.conv_clamp = conv_clamp\n",
        "        self.weight_gain = 1 / np.sqrt(in_channels * (kernel_size ** 2))\n",
        "        self.padding = kernel_size // 2\n",
        "\n",
        "        # Define weight parameter\n",
        "        self.weight = nn.Parameter(torch.randn([1, out_channels, in_channels, kernel_size, kernel_size]))\n",
        "\n",
        "        # Setup resample filter\n",
        "        self.register_buffer('resample_filter', torch.tensor(resample_filter, dtype=torch.float32))\n",
        "\n",
        "        # Use the existing FullyConnectedLayer for affine transformation\n",
        "        self.affine = FullyConnectedLayer(style_dim, in_channels, bias_init=1)\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        batch, in_channels, height, width = x.shape\n",
        "\n",
        "        # Modulation step using the pre-defined affine layer\n",
        "        style = self.affine(style).view(batch, 1, in_channels, 1, 1)\n",
        "        weight = self.weight * self.weight_gain * style\n",
        "\n",
        "        # Demodulation step\n",
        "        if self.demodulate:\n",
        "            decoefs = (weight.pow(2).sum(dim=[2, 3, 4]) + 1e-8).rsqrt()\n",
        "            weight = weight * decoefs.view(batch, self.out_channels, 1, 1, 1)\n",
        "\n",
        "        # Reshape weight and input for convolution\n",
        "        weight = weight.view(batch * self.out_channels, in_channels, self.kernel_size, self.kernel_size)\n",
        "        x = x.view(1, batch * in_channels, height, width)\n",
        "\n",
        "        # Apply convolution\n",
        "        out = F.conv2d(x, weight, padding=self.padding, groups=batch)\n",
        "\n",
        "        # Reshape output\n",
        "        out = out.view(batch, self.out_channels, *out.shape[2:])\n",
        "\n",
        "        # Clamp output if needed\n",
        "        if self.conv_clamp is not None:\n",
        "            out = out.clamp(-self.conv_clamp, self.conv_clamp)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "r3PxIze9-sUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "batch_size = 2\n",
        "in_channels = 3\n",
        "out_channels = 16\n",
        "kernel_size = 3\n",
        "style_dim = 512\n",
        "height, width = 64, 64\n",
        "\n",
        "modulated_conv = ModulatedConv2d(in_channels, out_channels, kernel_size, style_dim)\n",
        "x = torch.randn(batch_size, in_channels, height, width)\n",
        "style = torch.randn(batch_size, style_dim)\n",
        "\n",
        "output = modulated_conv(x, style)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33MLPI1tAJO4",
        "outputId": "9a80589e-0ebc-40f7-ccfb-6ac40d1806aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 16, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StyleConv\n",
        "The StyleConv class is designed to perform convolution operations while incorporating style modulation. It combines spatial information with style input, allowing for advanced style transfer applications in generative models."
      ],
      "metadata": {
        "id": "siegFl2PAK1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StyleConv(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,                    # Number of input channels.\n",
        "                 out_channels,                   # Number of output channels.\n",
        "                 style_dim,                      # Intermediate latent (W) dimensionality.\n",
        "                 resolution,                     # Resolution of this layer.\n",
        "                 kernel_size=3,                  # Convolution kernel size.\n",
        "                 up=1,                           # Integer upsampling factor.\n",
        "                 use_noise=True,                # Enable noise input?\n",
        "                 activation='lrelu',            # Activation function: 'relu', 'lrelu', etc.\n",
        "                 resample_filter=[1, 3, 3, 1],  # Low-pass filter for resampling activations.\n",
        "                 conv_clamp=None,               # Clamp output of convolution layers to +-X.\n",
        "                 demodulate=True,                # Perform demodulation.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the ModulatedConv2d layer\n",
        "        self.conv = ModulatedConv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            style_dim=style_dim,\n",
        "            demodulate=demodulate,\n",
        "            up=up,\n",
        "            resample_filter=resample_filter,\n",
        "            conv_clamp=conv_clamp\n",
        "        )\n",
        "\n",
        "        self.use_noise = use_noise\n",
        "        self.resolution = resolution\n",
        "        if use_noise:\n",
        "            self.register_buffer('noise_const', torch.randn([resolution, resolution]))\n",
        "            self.noise_strength = nn.Parameter(torch.zeros([]))\n",
        "\n",
        "        # Bias for the layer\n",
        "        self.bias = nn.Parameter(torch.zeros([out_channels]))\n",
        "\n",
        "        # Activation setup\n",
        "        self.activation = activation\n",
        "        self.act_gain = 1.0  # You can set this according to your requirements\n",
        "        self.conv_clamp = conv_clamp\n",
        "\n",
        "    def forward(self, x, style, noise_mode='random', gain=1):\n",
        "        # Apply the modulated convolution\n",
        "        x = self.conv(x, style)\n",
        "\n",
        "        # Ensure the noise mode is valid\n",
        "        assert noise_mode in ['random', 'const', 'none']\n",
        "\n",
        "        # Handle noise if enabled\n",
        "        if self.use_noise:\n",
        "            if noise_mode == 'random':\n",
        "                xh, xw = x.size()[-2:]\n",
        "                noise = torch.randn([x.shape[0], 1, xh, xw], device=x.device) * self.noise_strength\n",
        "            elif noise_mode == 'const':\n",
        "                noise = self.noise_const * self.noise_strength\n",
        "            else:  # noise_mode == 'none'\n",
        "                noise = 0\n",
        "\n",
        "            x = x + noise\n",
        "\n",
        "        # Apply bias, activation, gain, and clamping using the bias_act function\n",
        "        out = bias_act.bias_act(x, self.bias, act=self.activation, gain=self.act_gain, clamp=self.conv_clamp)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "CvH7JXPvAabK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "batch_size = 2\n",
        "in_channels = 3\n",
        "out_channels = 16\n",
        "style_dim = 512\n",
        "resolution = 64\n",
        "height, width = 64, 64\n",
        "\n",
        "style_conv = StyleConv(in_channels, out_channels, style_dim, resolution)\n",
        "x = torch.randn(batch_size, in_channels, height, width)\n",
        "style = torch.randn(batch_size, style_dim)\n",
        "\n",
        "output = style_conv(x, style)\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYyl06RKAhNR",
        "outputId": "8dff221e-5ade-4788-f3ac-e5235db2ce4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 16, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ToRGB\n",
        "This structure is typical in style-based image generation networks, where the ToRGB layer converts feature maps into RGB images, often incorporating style information for modulating the generated output."
      ],
      "metadata": {
        "id": "ERf4T3sUB-L7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ToRGB(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 style_dim,\n",
        "                 kernel_size=1,\n",
        "                 resample_filter=[1,3,3,1],\n",
        "                 conv_clamp=None,\n",
        "                 demodulate=False):\n",
        "        super(ToRGB, self).__init__()\n",
        "\n",
        "        self.conv = ModulatedConv2d(in_channels=in_channels,\n",
        "                                    out_channels=out_channels,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    style_dim=style_dim,\n",
        "                                    demodulate=demodulate,\n",
        "                                    resample_filter=resample_filter,\n",
        "                                    conv_clamp=conv_clamp)\n",
        "        self.bias = nn.Parameter(torch.zeros([out_channels]))\n",
        "        self.register_buffer('resample_filter', upfirdn2d.setup_filter(resample_filter))\n",
        "        self.conv_clamp = conv_clamp\n",
        "\n",
        "    def forward(self, x, style, skip=None):\n",
        "        x = self.conv(x, style)\n",
        "        out = bias_act.bias_act(x, self.bias, clamp=self.conv_clamp)\n",
        "\n",
        "        if skip is not None:\n",
        "            # If skip and output have different shapes, use interpolation to resize skip to match output size\n",
        "            if skip.shape != out.shape:\n",
        "                skip = torch.nn.functional.interpolate(skip, size=out.shape[2:], mode='bilinear', align_corners=False)\n",
        "            out = out + skip\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "wKFjaFRBB-ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some sample tensors\n",
        "input_tensor = torch.randn(1, 512, 16, 16).cuda()  # Random input tensor\n",
        "style_tensor = torch.randn(1, 512).cuda()  # Random style vector\n",
        "skip_tensor = torch.randn(1, 3, 64, 64).cuda()  # Skip connection (optional)\n",
        "\n",
        "# Create the ToRGB object\n",
        "rgb_layer = ToRGB(in_channels=512, out_channels=3, style_dim=512).cuda()\n",
        "\n",
        "# Run the forward pass\n",
        "output = rgb_layer(input_tensor, style_tensor, skip_tensor)\n",
        "print(output.shape)  # Output should be a tensor with shape (1, 3, 16, 16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLaUb-glCP_r",
        "outputId": "8026cc74-712a-48cc-f49a-1dd3a9e1ca88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... Failed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/MAT/torch_utils/ops/bias_act.py:50: UserWarning: Failed to build CUDA kernels for bias_act. Falling back to slow reference implementation. Details:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MAT/torch_utils/ops/bias_act.py\", line 48, in _init\n",
            "    _plugin = custom_ops.get_plugin('bias_act_plugin', sources=sources, extra_cuda_cflags=['--use_fast_math'])\n",
            "  File \"/content/MAT/torch_utils/custom_ops.py\", line 110, in get_plugin\n",
            "    torch.utils.cpp_extension.load(name=module_name, verbose=verbose_build, sources=sources, **build_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1314, in load\n",
            "    return _jit_compile(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1721, in _jit_compile\n",
            "    _write_ninja_file_and_build_library(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1803, in _write_ninja_file_and_build_library\n",
            "    verify_ninja_availability()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py\", line 1852, in verify_ninja_availability\n",
            "    raise RuntimeError(\"Ninja is required to load C++ extensions\")\n",
            "RuntimeError: Ninja is required to load C++ extensions\n",
            "\n",
            "  warnings.warn('Failed to build CUDA kernels for bias_act. Falling back to slow reference implementation. Details:\\n\\n' + traceback.format_exc())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_style_code(a, b):\n",
        "    return torch.cat([a, b], dim=1)"
      ],
      "metadata": {
        "id": "AMGGhcM93jzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DecBlockFirst"
      ],
      "metadata": {
        "id": "QZEjTPhm4UWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecBlockFirst(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels):\n",
        "        super().__init__()\n",
        "        # Fully connected layer to expand the latent vector into a 4x4 feature map\n",
        "        self.fc = FullyConnectedLayer(in_features=in_channels*2,\n",
        "                                      out_features=in_channels*4**2,\n",
        "                                      activation=activation)\n",
        "\n",
        "        # Conv layer to adjust the number of channels to out_channels (256)\n",
        "        self.adjust_channels = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "        # Style-based convolution\n",
        "        self.conv = StyleConv(in_channels=out_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              style_dim=style_dim,\n",
        "                              resolution=4,\n",
        "                              kernel_size=3,\n",
        "                              use_noise=use_noise,\n",
        "                              activation=activation,\n",
        "                              demodulate=demodulate,\n",
        "                              )\n",
        "        # ToRGB layer for converting the feature map into an RGB image\n",
        "        self.toRGB = ToRGB(in_channels=out_channels,\n",
        "                           out_channels=img_channels,\n",
        "                           style_dim=style_dim,\n",
        "                           kernel_size=1,\n",
        "                           demodulate=False,\n",
        "                           )\n",
        "\n",
        "    def forward(self, x, ws, gs, E_features, noise_mode='random'):\n",
        "        # Fully connected layer to reshape input latent vector into a 4x4 feature map\n",
        "        x = self.fc(x).view(x.shape[0], -1, 4, 4)\n",
        "\n",
        "        # Adjust the number of channels in x to match E_features\n",
        "        x = self.adjust_channels(x)\n",
        "\n",
        "        # Add skip connection (feature map from an encoder or earlier layer) to the feature map\n",
        "        x = x + E_features[2]  # Adds encoder feature map at a corresponding scale\n",
        "\n",
        "        # Get the style code for the first convolutional layer\n",
        "        style = get_style_code(ws[:, 0], gs)\n",
        "        x = self.conv(x, style, noise_mode=noise_mode)\n",
        "\n",
        "        # Get the style code for the ToRGB layer\n",
        "        style = get_style_code(ws[:, 1], gs)\n",
        "        img = self.toRGB(x, style, skip=None)\n",
        "\n",
        "        return x, img\n"
      ],
      "metadata": {
        "id": "W1khRkgG3ySe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "def get_style_code(ws, gs):\n",
        "    return ws + gs  # For testing, we just sum the style and global style vectors (simplified behavior)\n",
        "\n",
        "# Create an instance of DecBlockFirst\n",
        "in_channels = 512    # Example input channel size\n",
        "out_channels = 256   # Example output channel size\n",
        "style_dim = 512      # Example style vector dimension\n",
        "activation = 'relu'  # Example activation function\n",
        "use_noise = True     # Adding noise to the convolution\n",
        "demodulate = False   # Not using demodulation in the conv layer\n",
        "img_channels = 3     # Example image channels (RGB)\n",
        "\n",
        "dec_block_first = DecBlockFirst(in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels)\n",
        "\n",
        "# Create dummy inputs for testing the forward pass\n",
        "batch_size = 1  # Single batch\n",
        "x = torch.randn(batch_size, in_channels * 2)  # Latent vector for fully connected layer\n",
        "\n",
        "ws = torch.randn(batch_size, 2, style_dim)  # Style vectors for two stages (conv and ToRGB)\n",
        "gs = torch.randn(batch_size, style_dim)      # Global style vectors\n",
        "\n",
        "# E_features: List of features from an encoder (assuming 3 feature maps in the encoder)\n",
        "E_features = [torch.randn(batch_size, out_channels, 4, 4) for _ in range(3)]\n",
        "\n",
        "# Run the forward pass of DecBlockFirst\n",
        "x_out, img_out = dec_block_first(x, ws, gs, E_features)\n",
        "\n",
        "# Print the shape of the output tensors\n",
        "print(\"Output feature map shape:\", x_out.shape)  # Should be (batch_size, out_channels, 4, 4)\n",
        "print(\"Output image shape:\", img_out.shape)      # Should be (batch_size, img_channels, 4, 4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mg4nLpOu4iFA",
        "outputId": "8b259acf-1ceb-4fc8-f028-742cf051d265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output feature map shape: torch.Size([1, 256, 4, 4])\n",
            "Output image shape: torch.Size([1, 3, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecBlockFirstV2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels):\n",
        "        super().__init__()\n",
        "        self.conv0 = Conv2dLayer(in_channels=in_channels,\n",
        "                                out_channels=in_channels,\n",
        "                                kernel_size=3,\n",
        "                                activation=activation,\n",
        "                                )\n",
        "        self.conv1 = StyleConv(in_channels=in_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              style_dim=style_dim,\n",
        "                              resolution=4,\n",
        "                              kernel_size=3,\n",
        "                              use_noise=use_noise,\n",
        "                              activation=activation,\n",
        "                              demodulate=demodulate,\n",
        "                              )\n",
        "        self.toRGB = ToRGB(in_channels=out_channels,\n",
        "                           out_channels=img_channels,\n",
        "                           style_dim=style_dim,\n",
        "                           kernel_size=1,\n",
        "                           demodulate=False,\n",
        "                           )\n",
        "\n",
        "    def forward(self, x, ws, gs, E_features, noise_mode='random'):\n",
        "        x = self.conv0(x)\n",
        "        x = x + E_features[2]\n",
        "        style = get_style_code(ws[:, 0], gs)\n",
        "        x = self.conv1(x, style, noise_mode=noise_mode)\n",
        "        style = get_style_code(ws[:, 1], gs)\n",
        "        img = self.toRGB(x, style, skip=None)\n",
        "\n",
        "        return x, img\n"
      ],
      "metadata": {
        "id": "YVulV_cg4rjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming the following classes are defined:\n",
        "# Conv2dLayer, StyleConv, ToRGB, DecBlockFirstV2\n",
        "\n",
        "# Create an instance of DecBlockFirstV2\n",
        "in_channels = 512    # Example input channel size\n",
        "out_channels = 256   # Example output channel size\n",
        "style_dim = 512      # Example style vector dimension\n",
        "activation = 'relu'  # Example activation function\n",
        "use_noise = True     # Adding noise to the convolution\n",
        "demodulate = False   # Not using demodulation in the conv layer\n",
        "img_channels = 3     # Example image channels (RGB)\n",
        "\n",
        "dec_block_first_v2 = DecBlockFirstV2(in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels)\n",
        "\n",
        "# Create dummy inputs for testing the forward pass\n",
        "batch_size = 1  # Single batch\n",
        "x = torch.randn(batch_size, in_channels, 4, 4)  # Input feature map with correct shape\n",
        "\n",
        "ws = torch.randn(batch_size, 2, style_dim)  # Style vectors for two stages (conv and ToRGB)\n",
        "gs = torch.randn(batch_size, style_dim)      # Global style vectors\n",
        "\n",
        "# E_features: List of features from an encoder (assuming 3 feature maps in the encoder)\n",
        "# Note: Ensure these have the right shape and number of channels\n",
        "E_features = [torch.randn(batch_size, in_channels, 4, 4) for _ in range(3)]  # Change `out_channels` to `in_channels`\n",
        "\n",
        "# Run the forward pass of DecBlockFirstV2\n",
        "x_out, img_out = dec_block_first_v2(x, ws, gs, E_features)\n",
        "\n",
        "# Print the shape of the output tensors\n",
        "print(\"Output feature map shape:\", x_out.shape)  # Should be (batch_size, out_channels, 4, 4)\n",
        "print(\"Output image shape:\", img_out.shape)      # Should be (batch_size, img_channels, 4, 4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIQyNyNf5bsc",
        "outputId": "99008f92-494a-484a-fb1b-5930e0df1e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output feature map shape: torch.Size([1, 256, 4, 4])\n",
            "Output image shape: torch.Size([1, 3, 4, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecBlock(nn.Module):\n",
        "    def __init__(self, res, in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels):  # res = 2, ..., resolution_log2\n",
        "        super().__init__()\n",
        "        self.res = res\n",
        "\n",
        "        self.conv0 = StyleConv(in_channels=in_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               style_dim=style_dim,\n",
        "                               resolution=2**res,\n",
        "                               kernel_size=3,\n",
        "                               up=2,\n",
        "                               use_noise=use_noise,\n",
        "                               activation=activation,\n",
        "                               demodulate=demodulate,\n",
        "                               )\n",
        "        self.conv1 = StyleConv(in_channels=out_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               style_dim=style_dim,\n",
        "                               resolution=2**res,\n",
        "                               kernel_size=3,\n",
        "                               use_noise=use_noise,\n",
        "                               activation=activation,\n",
        "                               demodulate=demodulate,\n",
        "                               )\n",
        "        self.toRGB = ToRGB(in_channels=out_channels,\n",
        "                           out_channels=img_channels,\n",
        "                           style_dim=style_dim,\n",
        "                           kernel_size=1,\n",
        "                           demodulate=False,\n",
        "                           )\n",
        "\n",
        "    def forward(self, x, img, ws, gs, E_features, noise_mode='random'):\n",
        "        style = get_style_code(ws[:, self.res * 2 - 5], gs)\n",
        "        x = self.conv0(x, style, noise_mode=noise_mode)\n",
        "        x = x + E_features[self.res]\n",
        "        style = get_style_code(ws[:, self.res * 2 - 4], gs)\n",
        "        x = self.conv1(x, style, noise_mode=noise_mode)\n",
        "        style = get_style_code(ws[:, self.res * 2 - 3], gs)\n",
        "        img = self.toRGB(x, style, skip=img)\n",
        "\n",
        "        return x, img"
      ],
      "metadata": {
        "id": "fozSfFuP5eLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MappingNet"
      ],
      "metadata": {
        "id": "L2lpA46W6yR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_2nd_moment(x, dim=1, eps=1e-8):\n",
        "    return x * (x.square().mean(dim=dim, keepdim=True) + eps).rsqrt()"
      ],
      "metadata": {
        "id": "GyufLJ-v7SGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingNet(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "        z_dim,                      # Input latent (Z) dimensionality, 0 = no latent.\n",
        "        c_dim,                      # Conditioning label (C) dimensionality, 0 = no label.\n",
        "        w_dim,                      # Intermediate latent (W) dimensionality.\n",
        "        num_ws,                     # Number of intermediate latents to output, None = do not broadcast.\n",
        "        num_layers      = 8,        # Number of mapping layers.\n",
        "        embed_features  = None,     # Label embedding dimensionality, None = same as w_dim.\n",
        "        layer_features  = None,     # Number of intermediate features in the mapping layers, None = same as w_dim.\n",
        "        activation      = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "        lr_multiplier   = 0.01,     # Learning rate multiplier for the mapping layers.\n",
        "        w_avg_beta      = 0.995,    # Decay for tracking the moving average of W during training, None = do not track.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.num_ws = num_ws\n",
        "        self.num_layers = num_layers\n",
        "        self.w_avg_beta = w_avg_beta\n",
        "\n",
        "        if embed_features is None:\n",
        "            embed_features = w_dim\n",
        "        if c_dim == 0:\n",
        "            embed_features = 0\n",
        "        if layer_features is None:\n",
        "            layer_features = w_dim\n",
        "        features_list = [z_dim + embed_features] + [layer_features] * (num_layers - 1) + [w_dim]\n",
        "\n",
        "        if c_dim > 0:\n",
        "            self.embed = FullyConnectedLayer(c_dim, embed_features)\n",
        "        for idx in range(num_layers):\n",
        "            in_features = features_list[idx]\n",
        "            out_features = features_list[idx + 1]\n",
        "            layer = FullyConnectedLayer(in_features, out_features, activation=activation, lr_multiplier=lr_multiplier)\n",
        "            setattr(self, f'fc{idx}', layer)\n",
        "\n",
        "        if num_ws is not None and w_avg_beta is not None:\n",
        "            self.register_buffer('w_avg', torch.zeros([w_dim]))\n",
        "\n",
        "    def forward(self, z, c, truncation_psi=1, truncation_cutoff=None, skip_w_avg_update=False):\n",
        "        # Embed, normalize, and concat inputs.\n",
        "        x = None\n",
        "        with torch.autograd.profiler.record_function('input'):\n",
        "            if self.z_dim > 0:\n",
        "                x = normalize_2nd_moment(z.to(torch.float32))\n",
        "            if self.c_dim > 0:\n",
        "                c = c.view(-1, 1).expand(-1, self.c_dim)  # or c.unsqueeze(1) depending on your setup\n",
        "                y = normalize_2nd_moment(self.embed(c.to(torch.float32)))\n",
        "                x = torch.cat([x, y], dim=1) if x is not None else y\n",
        "\n",
        "        # Main layers.\n",
        "        for idx in range(self.num_layers):\n",
        "            layer = getattr(self, f'fc{idx}')\n",
        "            x = layer(x)\n",
        "\n",
        "        # Update moving average of W.\n",
        "        if self.w_avg_beta is not None and self.training and not skip_w_avg_update:\n",
        "            with torch.autograd.profiler.record_function('update_w_avg'):\n",
        "                self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))\n",
        "\n",
        "        # Broadcast.\n",
        "        if self.num_ws is not None:\n",
        "            with torch.autograd.profiler.record_function('broadcast'):\n",
        "                x = x.unsqueeze(1).repeat([1, self.num_ws, 1])\n",
        "\n",
        "        # Apply truncation.\n",
        "        if truncation_psi != 1:\n",
        "            with torch.autograd.profiler.record_function('truncate'):\n",
        "                assert self.w_avg_beta is not None\n",
        "                if self.num_ws is None or truncation_cutoff is None:\n",
        "                    x = self.w_avg.lerp(x, truncation_psi)\n",
        "                else:\n",
        "                    x[:, :truncation_cutoff] = self.w_avg.lerp(x[:, :truncation_cutoff], truncation_psi)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "bXQFudVN6wxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example parameters\n",
        "z_dim = 512              # Latent vector dimension\n",
        "c_dim = 10               # Conditioning dimension (e.g., class labels)\n",
        "w_dim = 512              # Intermediate latent space dimension\n",
        "num_ws = 8               # Number of output latents\n",
        "num_layers = 8           # Number of layers in the mapping network\n",
        "\n",
        "# Create the mapping network\n",
        "mapping_net = MappingNet(z_dim, c_dim, w_dim, num_ws, num_layers)\n",
        "\n",
        "# Example inputs\n",
        "batch_size = 4\n",
        "z = torch.randn(batch_size, z_dim)  # Random latent vectors\n",
        "c = torch.randint(0, c_dim, (batch_size,))  # Random conditioning labels\n",
        "\n",
        "# Forward pass\n",
        "output = mapping_net(z, c)\n",
        "\n",
        "# Print output shape\n",
        "print(\"Output shape:\", output.shape)  # Should be (batch_size, num_ws, w_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3O3zQEq6-Q8",
        "outputId": "2a1f2226-93df-45b3-e653-b37b6406a40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([4, 8, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DisFromRGB"
      ],
      "metadata": {
        "id": "SVfFw4zn7w2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DisFromRGB(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation):  # res = 2, ..., resolution_log2\n",
        "        super().__init__()\n",
        "        self.conv = Conv2dLayer(in_channels=in_channels,\n",
        "                                out_channels=out_channels,\n",
        "                                kernel_size=1,\n",
        "                                activation=activation,\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "metadata": {
        "id": "BEdftXm07BwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DisBlock"
      ],
      "metadata": {
        "id": "G9A0Tm_K72b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DisBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation):  # res = 2, ..., resolution_log2\n",
        "        super().__init__()\n",
        "        self.conv0 = Conv2dLayer(in_channels=in_channels,\n",
        "                                 out_channels=in_channels,\n",
        "                                 kernel_size=3,\n",
        "                                 activation=activation,\n",
        "                                 )\n",
        "        self.conv1 = Conv2dLayer(in_channels=in_channels,\n",
        "                                 out_channels=out_channels,\n",
        "                                 kernel_size=3,\n",
        "                                 down=2,\n",
        "                                 activation=activation,\n",
        "                                 )\n",
        "        self.skip = Conv2dLayer(in_channels=in_channels,\n",
        "                                out_channels=out_channels,\n",
        "                                kernel_size=1,\n",
        "                                down=2,\n",
        "                                bias=False,\n",
        "                             )\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = self.skip(x, gain=np.sqrt(0.5))\n",
        "        x = self.conv0(x)\n",
        "        x = self.conv1(x, gain=np.sqrt(0.5))\n",
        "        out = skip + x\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "_9H_3Otn7x0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 512\n",
        "out_channels = 256\n",
        "activation = 'relu'\n",
        "\n",
        "dis_block = DisBlock(in_channels, out_channels, activation)\n",
        "\n",
        "# Create a dummy input tensor\n",
        "batch_size = 2\n",
        "height = 8\n",
        "width = 8\n",
        "x = torch.randn(batch_size, in_channels, height, width)\n",
        "\n",
        "# Forward pass\n",
        "output = dis_block(x)\n",
        "\n",
        "# Print output shape\n",
        "print(x.shape)\n",
        "print(\"Output shape:\", output.shape)  # Should be (batch_size, out_channels, height // 2, width // 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIVbtlpj78y2",
        "outputId": "7131be8c-d7fa-43fe-9e28-635152d19b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 512, 8, 8])\n",
            "Output shape: torch.Size([2, 256, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MiniBatchStdLayer"
      ],
      "metadata": {
        "id": "LS_0m8sp8vn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinibatchStdLayer(torch.nn.Module):\n",
        "    def __init__(self, group_size, num_channels=1):\n",
        "        super().__init__()\n",
        "        self.group_size = group_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        # Calculate the effective group size\n",
        "        G = min(self.group_size, N) if self.group_size is not None else N\n",
        "        F = self.num_channels\n",
        "        c = C // F\n",
        "\n",
        "        # Reshape the input tensor into groups\n",
        "        y = x.reshape(G, -1, F, c, H, W)  # [GnFcHW]\n",
        "        y = y - y.mean(dim=0)  # Subtract mean over group\n",
        "        y = y.square().mean(dim=0)  # Calculate variance over group\n",
        "        y = (y + 1e-8).sqrt()  # Calculate stddev over group\n",
        "        y = y.mean(dim=[2, 3, 4])  # Average over channels and pixels\n",
        "        y = y.reshape(-1, F, 1, 1)  # Add missing dimensions\n",
        "        y = y.repeat(G, 1, H, W)  # Replicate over group and pixels\n",
        "\n",
        "        # Concatenate the stddev to the input\n",
        "        x = torch.cat([x, y], dim=1)  # [NCHW]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "vNrrSYiK8CU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "batch_size = 4  # Number of samples in a batch\n",
        "channels = 3    # Number of input channels (e.g., RGB)\n",
        "height = 8      # Height of the feature maps\n",
        "width = 8       # Width of the feature maps\n",
        "\n",
        "# Create a random input tensor\n",
        "x = torch.randn(batch_size, channels, height, width)\n",
        "\n",
        "# Create an instance of MinibatchStdLayer\n",
        "minibatch_std_layer = MinibatchStdLayer(group_size=2, num_channels=1)\n",
        "\n",
        "# Forward pass\n",
        "output = minibatch_std_layer(x)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PQSUNvR80Eg",
        "outputId": "0864c516-6167-4cc3-ee71-6b34025423f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([4, 4, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discriminator"
      ],
      "metadata": {
        "id": "oSTm9biX9Y1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 c_dim,                        # Conditioning label (C) dimensionality.\n",
        "                 img_resolution,               # Input resolution.\n",
        "                 img_channels,                 # Number of input color channels.\n",
        "                 channel_base       = 32768,    # Overall multiplier for the number of channels.\n",
        "                 channel_max        = 512,      # Maximum number of channels in any layer.\n",
        "                 channel_decay      = 1,\n",
        "                 cmap_dim           = None,     # Dimensionality of mapped conditioning label, None = default.\n",
        "                 activation         = 'lrelu',\n",
        "                 mbstd_group_size   = 4,        # Group size for the minibatch standard deviation layer, None = entire minibatch.\n",
        "                 mbstd_num_channels = 1,        # Number of features for the minibatch standard deviation layer, 0 = disable.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.c_dim = c_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "\n",
        "        resolution_log2 = int(np.log2(img_resolution))\n",
        "        assert img_resolution == 2 ** resolution_log2 and img_resolution >= 4\n",
        "        self.resolution_log2 = resolution_log2\n",
        "\n",
        "        def nf(stage):\n",
        "            return np.clip(int(channel_base / 2 ** (stage * channel_decay)), 1, channel_max)\n",
        "\n",
        "        if cmap_dim == None:\n",
        "            cmap_dim = nf(2)\n",
        "        if c_dim == 0:\n",
        "            cmap_dim = 0\n",
        "        self.cmap_dim = cmap_dim\n",
        "\n",
        "        if c_dim > 0:\n",
        "            self.mapping = MappingNet(z_dim=0, c_dim=c_dim, w_dim=cmap_dim, num_ws=None, w_avg_beta=None)\n",
        "\n",
        "        Dis = [DisFromRGB(img_channels+1, nf(resolution_log2), activation)]\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            Dis.append(DisBlock(nf(res), nf(res-1), activation))\n",
        "\n",
        "        if mbstd_num_channels > 0:\n",
        "            Dis.append(MinibatchStdLayer(group_size=mbstd_group_size, num_channels=mbstd_num_channels))\n",
        "        Dis.append(Conv2dLayer(nf(2) + mbstd_num_channels, nf(2), kernel_size=3, activation=activation))\n",
        "        self.Dis = nn.Sequential(*Dis)\n",
        "\n",
        "        self.fc0 = FullyConnectedLayer(nf(2)*4**2, nf(2), activation=activation)\n",
        "        self.fc1 = FullyConnectedLayer(nf(2), 1 if cmap_dim == 0 else cmap_dim)\n",
        "\n",
        "    def forward(self, images_in, masks_in, c):\n",
        "        x = torch.cat([masks_in - 0.5, images_in], dim=1)\n",
        "        x = self.Dis(x)\n",
        "        x = self.fc1(self.fc0(x.flatten(start_dim=1)))\n",
        "\n",
        "        if self.c_dim > 0:\n",
        "            cmap = self.mapping(None, c)\n",
        "\n",
        "        if self.cmap_dim > 0:\n",
        "            x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "cO3IItUe87CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nf(stage, channel_base=32768, channel_decay=1.0, channel_max=512):\n",
        "    NF = {512: 64, 256: 128, 128: 256, 64: 512, 32: 512, 16: 512, 8: 512, 4: 512}\n",
        "    return NF[2 ** stage]"
      ],
      "metadata": {
        "id": "IaNQdMPX7yEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP layer"
      ],
      "metadata": {
        "id": "cCOV5yZL6i1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Mlp(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = FullyConnectedLayer(in_features=in_features, out_features=hidden_features, activation='lrelu')\n",
        "        self.fc2 = FullyConnectedLayer(in_features=hidden_features, out_features=out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8oAjr5a-6ibC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "# Input feature size of 128, hidden size of 64, output size of 32\n",
        "# Using a dropout of 0.5 and GELU activation\n",
        "mlp_model = Mlp(in_features=128, hidden_features=64, out_features=32, act_layer=torch.nn.GELU, drop=0.5)\n",
        "\n",
        "# Create a random tensor to simulate a batch of inputs\n",
        "# Assuming a batch size of 10 with 128 features per input\n",
        "input_tensor = torch.randn(10, 128)\n",
        "\n",
        "# Perform a forward pass through the model\n",
        "output_tensor = mlp_model(input_tensor)\n",
        "\n",
        "# Print output shape and values for verification\n",
        "print(\"Output shape:\", output_tensor.shape)  # Expected shape: (10, 32)\n",
        "print(\"Output values:\", output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-7_H-76qJi",
        "outputId": "90748fc9-3f26-4ba3-bed0-8fabc28cfcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([10, 32])\n",
            "Output values: tensor([[ 1.9070, -1.8160,  0.3538,  0.5282,  1.0192,  0.3757,  0.2851, -0.6292,\n",
            "         -1.3916,  1.7813,  0.6381,  0.4187, -1.1728,  1.0477, -0.4216,  1.4196,\n",
            "          0.0097,  0.5228, -0.9435,  1.2384,  0.3905, -0.9183,  2.0544,  0.1904,\n",
            "         -0.8917,  0.0949,  2.1079,  0.1109, -0.6154,  0.5166, -0.7709,  0.0137],\n",
            "        [-0.9681, -1.1288,  0.3563,  0.6750, -0.0262, -0.8574, -0.1819, -1.4985,\n",
            "          0.2216,  1.0489, -0.9350, -1.0763, -0.8498, -1.4554,  0.3384,  0.6307,\n",
            "         -0.3702, -1.3922, -0.1444, -2.0599,  0.7344,  1.4922, -0.7892,  1.1897,\n",
            "         -0.6544,  1.5134, -0.6282, -1.5124,  1.0093, -0.0036,  0.5671,  0.2046],\n",
            "        [ 0.8011, -1.3428,  0.3524,  1.5907,  0.9532,  1.9668, -0.5615, -0.6193,\n",
            "         -0.8348, -0.2388, -0.7735,  0.2250, -2.3748, -1.3422, -1.3789, -0.5028,\n",
            "          1.4112,  0.2426,  0.4453, -0.6862, -1.2582, -1.6452,  1.4368,  1.6692,\n",
            "          0.4263, -0.2722,  2.1582,  1.4501, -0.9480, -0.8974, -1.4251,  0.9711],\n",
            "        [-1.9577,  0.3236, -0.0645,  0.6372,  0.0516, -1.2085,  0.3082, -0.3163,\n",
            "          0.0206,  1.0078,  0.2791, -1.5867, -0.6624, -2.1190,  0.1056, -0.1647,\n",
            "          0.0107, -0.4750, -0.1288, -0.5983, -0.6801,  1.3813,  0.6110, -0.8255,\n",
            "          0.2083,  0.8956, -0.2778,  0.2105,  1.1027,  0.3199, -0.9164,  0.3487],\n",
            "        [-0.7341,  0.7170,  1.5249,  0.7602,  1.1486,  0.9199,  0.1294, -0.5235,\n",
            "         -1.3871,  0.6880, -0.4165,  0.2538, -1.0604,  0.7389,  0.8831, -0.2594,\n",
            "          0.7514, -0.1994, -1.3825,  1.4836,  0.2984,  0.0094,  0.7224, -0.2528,\n",
            "          0.7026, -0.0643,  0.7830,  0.0189, -0.7075, -0.3023, -1.4331, -1.0348],\n",
            "        [ 0.1109, -0.8870,  1.5431,  1.6981, -0.2869,  1.4324, -1.3777, -0.6521,\n",
            "         -1.4144,  0.2160,  0.2793, -0.8999, -1.7168, -1.0921, -1.1140,  1.4468,\n",
            "          0.9307,  1.7729, -0.8293,  0.0347,  1.4909,  0.9379, -0.4280, -1.3436,\n",
            "         -1.6248, -0.7784,  1.2098,  0.2437,  0.3469, -0.6579, -0.5443, -0.3149],\n",
            "        [ 0.5812, -1.0341,  0.3367,  0.9600, -0.1349,  0.9272,  0.3613, -0.3096,\n",
            "         -1.1543,  0.1175, -0.6586,  0.7290, -2.2062,  2.0526, -1.4637,  0.2804,\n",
            "         -1.9137,  0.8763,  1.0385,  1.6406, -0.2977,  0.3084, -0.1264,  1.6101,\n",
            "         -0.7411,  0.2593,  1.9932, -0.9039,  0.3605,  1.5258,  1.4957,  0.0111],\n",
            "        [-0.9156, -1.9348,  0.8185,  1.3462, -2.0441,  1.4788,  0.1169, -0.2657,\n",
            "         -1.0953,  0.1805, -0.4937, -1.5076, -0.7792,  0.4505, -2.7732, -1.1866,\n",
            "          0.4332,  0.5276, -1.0668,  0.6925,  1.2827,  0.4321, -0.2662,  0.0935,\n",
            "         -0.2896,  0.2820,  0.2634,  0.3560,  0.6888, -0.3496, -0.9339, -0.8069],\n",
            "        [-0.1112, -0.1839, -1.5460,  0.2225,  1.1347, -0.1041,  0.1802,  0.2615,\n",
            "         -1.4855,  1.4126, -0.1309,  0.1272, -0.3931, -0.7631,  0.9509,  1.2276,\n",
            "          0.3189, -0.3600, -0.1522,  0.7206,  1.5016,  0.4509,  1.4175,  0.0175,\n",
            "         -0.0890,  0.0830, -0.2113, -0.1512,  0.1079, -0.2324, -0.5062, -0.8506],\n",
            "        [ 0.6706, -0.2394,  1.4074,  0.9218,  0.0262,  0.3822,  0.2925,  0.6209,\n",
            "         -1.7550,  0.6896,  0.5521, -0.0690,  0.0676,  0.8204,  1.0702, -0.1329,\n",
            "          0.0152,  0.5285,  0.8086, -0.0964,  0.3810, -1.2739, -1.4800,  0.0800,\n",
            "         -1.6068, -0.9870, -0.6666,  0.7854, -0.4969, -0.0146,  0.4393, -0.5120]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "window partition"
      ],
      "metadata": {
        "id": "TK3ciqeg64nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size (int): window size\n",
        "    Returns:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows"
      ],
      "metadata": {
        "id": "D82uDq3P6_I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "import torch\n",
        "\n",
        "# Example input tensor of shape (Batch, Height, Width, Channels)\n",
        "B, H, W, C = 2, 8, 8, 3  # 1 batch, 8x8 grid, 3 channels\n",
        "x = torch.arange(B * H * W * C).view(B, H, W, C).float()\n",
        "\n",
        "window_size = 4  # Specify window size\n",
        "\n",
        "# Use the window_partition function\n",
        "windows = window_partition(x, window_size)\n",
        "\n",
        "print(\"Input tensor shape:\", x.shape)\n",
        "print(\"Partitioned windows shape:\", windows.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVTItXvC7AQw",
        "outputId": "6cd4ac48-b5f2-4fff-976c-76c772bc4367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tensor shape: torch.Size([2, 8, 8, 3])\n",
            "Partitioned windows shape: torch.Size([8, 4, 4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window reverse"
      ],
      "metadata": {
        "id": "L-uJImq77Z0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: (num_windows*B, window_size, window_size, C)\n",
        "        window_size (int): Window size\n",
        "        H (int): Height of image\n",
        "        W (int): Width of image\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "PV9ldVaq7GH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "import torch\n",
        "\n",
        "# Set parameters for the test\n",
        "B, H, W, C = 2, 8, 8, 3    # Batch size, image height, width, and channels\n",
        "window_size = 4            # Window size\n",
        "\n",
        "# Create a sample tensor and partition it into windows\n",
        "input_tensor = torch.randn(B, H, W, C)\n",
        "windows = window_partition(input_tensor, window_size)\n",
        "\n",
        "# Reconstruct the image using window_reverse\n",
        "output_tensor = window_reverse(windows, window_size, H, W)\n",
        "\n",
        "# Print output shapes and check if they match the original input\n",
        "print(\"Original shape:\", input_tensor.shape)         # Expected shape: (B, H, W, C)\n",
        "print(\"Reconstructed shape:\", output_tensor.shape)    # Expected shape: (B, H, W, C)\n",
        "\n",
        "# Verify if reconstruction matches the original input\n",
        "if torch.allclose(input_tensor, output_tensor):\n",
        "    print(\"The reconstruction is correct!\")\n",
        "else:\n",
        "    print(\"The reconstruction differs from the original input.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1R8zrwQ7c5a",
        "outputId": "f11559a1-767a-48cc-c0d6-b9458b928cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: torch.Size([2, 8, 8, 3])\n",
            "Reconstructed shape: torch.Size([2, 8, 8, 3])\n",
            "The reconstruction is correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conv2dLayerPartial"
      ],
      "metadata": {
        "id": "2CcOo9j070kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2dLayerPartial(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,                    # Number of input channels.\n",
        "                 out_channels,                   # Number of output channels.\n",
        "                 kernel_size,                    # Width and height of the convolution kernel.\n",
        "                 bias            = True,         # Apply additive bias before the activation function?\n",
        "                 activation      = 'linear',     # Activation function: 'relu', 'lrelu', etc.\n",
        "                 up              = 1,            # Integer upsampling factor.\n",
        "                 down            = 1,            # Integer downsampling factor.\n",
        "                 resample_filter = [1,3,3,1],    # Low-pass filter to apply when resampling activations.\n",
        "                 conv_clamp      = None,         # Clamp the output to +-X, None = disable clamping.\n",
        "                 trainable       = True,         # Update the weights of this layer during training?\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.conv = Conv2dLayer(in_channels, out_channels, kernel_size, bias, activation, up, down, resample_filter,\n",
        "                                conv_clamp, trainable)\n",
        "\n",
        "        self.weight_maskUpdater = torch.ones(1, 1, kernel_size, kernel_size)\n",
        "        self.slide_winsize = kernel_size ** 2\n",
        "        self.stride = down\n",
        "        self.padding = kernel_size // 2 if kernel_size % 2 == 1 else 0\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        if mask is not None:\n",
        "            with torch.no_grad():\n",
        "                if self.weight_maskUpdater.type() != x.type():\n",
        "                    self.weight_maskUpdater = self.weight_maskUpdater.to(x)\n",
        "                update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding)\n",
        "                mask_ratio = self.slide_winsize / (update_mask + 1e-8)\n",
        "                update_mask = torch.clamp(update_mask, 0, 1)  # 0 or 1\n",
        "                mask_ratio = torch.mul(mask_ratio, update_mask)\n",
        "            x = self.conv(x)\n",
        "            x = torch.mul(x, mask_ratio)\n",
        "            return x, update_mask\n",
        "        else:\n",
        "            x = self.conv(x)\n",
        "            return x, None"
      ],
      "metadata": {
        "id": "BaAn8JIe7hBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Initialize Conv2dLayerPartial layer\n",
        "conv_layer = Conv2dLayerPartial(in_channels=3, out_channels=3, kernel_size=3, activation='lrelu')\n",
        "\n",
        "# Create an input tensor and a mask\n",
        "input_tensor = torch.randn(1, 3, 8, 8)      # Batch size = 1, Channels = 3, Height = 8, Width = 8\n",
        "mask = torch.ones(1, 1, 8, 8)               # Mask tensor, with same spatial dimensions as input\n",
        "\n",
        "# Add a \"hole\" in the mask for testing\n",
        "mask[:, :, 3:5, 3:5] = 0  # Set part of the mask to zero\n",
        "\n",
        "# Forward pass with the mask\n",
        "output, updated_mask = conv_layer(input_tensor, mask)\n",
        "\n",
        "# Print output and mask shapes\n",
        "print(\"Output shape:\", output.shape)               # Expected: (1, 3, 8, 8)\n",
        "print(\"Updated mask shape:\", updated_mask.shape)    # Expected: (1, 1, 8, 8)\n",
        "print(\"Updated mask:\\n\", updated_mask[0, 0].cpu())  # Display the updated mask for verification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyG5wyMp8DHC",
        "outputId": "f1b83ca8-8b77-415a-f030-398f4d54439d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 3, 8, 8])\n",
            "Updated mask shape: torch.Size([1, 1, 8, 8])\n",
            "Updated mask:\n",
            " tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " Window based multi-head self attention (W-MSA) module\n"
      ],
      "metadata": {
        "id": "Vi5dsZ4e8SEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowAttention(nn.Module):\n",
        "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
        "    It supports both of shifted and non-shifted window.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        window_size (tuple[int]): The height and width of the window.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
        "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
        "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, window_size, num_heads, down_ratio=1, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.window_size = window_size  # Wh, Ww\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.q = FullyConnectedLayer(in_features=dim, out_features=dim)\n",
        "        self.k = FullyConnectedLayer(in_features=dim, out_features=dim)\n",
        "        self.v = FullyConnectedLayer(in_features=dim, out_features=dim)\n",
        "        self.proj = FullyConnectedLayer(in_features=dim, out_features=dim)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask_windows=None, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input features with shape of (num_windows*B, N, C)\n",
        "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
        "        \"\"\"\n",
        "        B_, N, C = x.shape\n",
        "        norm_x = F.normalize(x, p=2.0, dim=-1)\n",
        "        q = self.q(norm_x).reshape(B_, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        k = self.k(norm_x).view(B_, -1, self.num_heads, C // self.num_heads).permute(0, 2, 3, 1)\n",
        "        v = self.v(x).view(B_, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q @ k) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            nW = mask.shape[0]\n",
        "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
        "            attn = attn.view(-1, self.num_heads, N, N)\n",
        "\n",
        "        if mask_windows is not None:\n",
        "            attn_mask_windows = mask_windows.squeeze(-1).unsqueeze(1).unsqueeze(1)\n",
        "            attn = attn + attn_mask_windows.masked_fill(attn_mask_windows == 0, float(-100.0)).masked_fill(\n",
        "                attn_mask_windows == 1, float(0.0))\n",
        "            with torch.no_grad():\n",
        "                mask_windows = torch.clamp(torch.sum(mask_windows, dim=1, keepdim=True), 0, 1).repeat(1, N, 1)\n",
        "\n",
        "        attn = self.softmax(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x, mask_windows\n"
      ],
      "metadata": {
        "id": "mvvaErQj8JOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define parameters\n",
        "dim = 64               # Input dimension\n",
        "window_size = (4, 4)   # Window height and width\n",
        "num_heads = 8          # Number of attention heads\n",
        "batch_size = 2         # Batch size\n",
        "num_windows = 16       # Number of windows\n",
        "N = window_size[0] * window_size[1]  # Number of tokens per window (assuming square windows)\n",
        "\n",
        "# Initialize WindowAttention module\n",
        "window_attention = WindowAttention(dim=dim, window_size=window_size, num_heads=num_heads)\n",
        "\n",
        "# Create dummy input tensor\n",
        "x = torch.randn(num_windows * batch_size, N, dim)  # Shape: (num_windows * batch_size, N, dim)\n",
        "\n",
        "# Forward pass through the module\n",
        "output, mask_windows = window_attention(x)\n",
        "\n",
        "# Print output shapes\n",
        "print(\"Output shape:\", output.shape)          # Expected: (num_windows * batch_size, N, dim)\n",
        "if mask_windows is not None:\n",
        "    print(\"Mask windows shape:\", mask_windows.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb4UqZ0h8S_u",
        "outputId": "4b65d825-eee5-49a2-d885-af81487f0042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([32, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swin Transformer Block"
      ],
      "metadata": {
        "id": "iYIrkUKd80Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinTransformerBlock(nn.Module):\n",
        "    r\"\"\" Swin Transformer Block.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resulotion.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Window size.\n",
        "        shift_size (int): Shift size for SW-MSA.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
        "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, num_heads, down_ratio=1, window_size=7, shift_size=0,\n",
        "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.num_heads = num_heads\n",
        "        self.window_size = window_size\n",
        "        self.shift_size = shift_size\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        if min(self.input_resolution) <= self.window_size:\n",
        "            # if window size is larger than input resolution, we don't partition windows\n",
        "            self.shift_size = 0\n",
        "            self.window_size = min(self.input_resolution)\n",
        "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            down_ratio = 1\n",
        "        self.attn = WindowAttention(dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
        "                                    down_ratio=down_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop,\n",
        "                                    proj_drop=drop)\n",
        "\n",
        "        self.fuse = FullyConnectedLayer(in_features=dim * 2, out_features=dim, activation='lrelu')\n",
        "\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
        "\n",
        "        if self.shift_size > 0:\n",
        "            attn_mask = self.calculate_mask(self.input_resolution)\n",
        "        else:\n",
        "            attn_mask = None\n",
        "\n",
        "        self.register_buffer(\"attn_mask\", attn_mask)\n",
        "\n",
        "    def calculate_mask(self, x_size):\n",
        "        # calculate attention mask for SW-MSA\n",
        "        H, W = x_size\n",
        "        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
        "        h_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        w_slices = (slice(0, -self.window_size),\n",
        "                    slice(-self.window_size, -self.shift_size),\n",
        "                    slice(-self.shift_size, None))\n",
        "        cnt = 0\n",
        "        for h in h_slices:\n",
        "            for w in w_slices:\n",
        "                img_mask[:, h, w, :] = cnt\n",
        "                cnt += 1\n",
        "\n",
        "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
        "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
        "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
        "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
        "\n",
        "        return attn_mask\n",
        "\n",
        "    def forward(self, x, x_size, mask=None):\n",
        "        # H, W = self.input_resolution\n",
        "        H, W = x_size\n",
        "        B, L, C = x.shape\n",
        "        assert L == H * W, \"input feature has wrong size\"\n",
        "\n",
        "        shortcut = x\n",
        "        x = x.view(B, H, W, C)\n",
        "        if mask is not None:\n",
        "            mask = mask.view(B, H, W, 1)\n",
        "\n",
        "        # cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "            if mask is not None:\n",
        "                shifted_mask = torch.roll(mask, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            shifted_x = x\n",
        "            if mask is not None:\n",
        "                shifted_mask = mask\n",
        "\n",
        "        # partition windows\n",
        "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
        "        if mask is not None:\n",
        "            mask_windows = window_partition(shifted_mask, self.window_size)\n",
        "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size, 1)\n",
        "        else:\n",
        "            mask_windows = None\n",
        "\n",
        "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
        "        if self.input_resolution == x_size:\n",
        "            attn_windows, mask_windows = self.attn(x_windows, mask_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
        "        else:\n",
        "            attn_windows, mask_windows = self.attn(x_windows, mask_windows, mask=self.calculate_mask(x_size).to(x.device))  # nW*B, window_size*window_size, C\n",
        "\n",
        "        # merge windows\n",
        "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
        "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
        "        if mask is not None:\n",
        "            mask_windows = mask_windows.view(-1, self.window_size, self.window_size, 1)\n",
        "            shifted_mask = window_reverse(mask_windows, self.window_size, H, W)\n",
        "\n",
        "        # reverse cyclic shift\n",
        "        if self.shift_size > 0:\n",
        "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "            if mask is not None:\n",
        "                mask = torch.roll(shifted_mask, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
        "        else:\n",
        "            x = shifted_x\n",
        "            if mask is not None:\n",
        "                mask = shifted_mask\n",
        "        x = x.view(B, H * W, C)\n",
        "        if mask is not None:\n",
        "            mask = mask.view(B, H * W, 1)\n",
        "\n",
        "        # FFN\n",
        "        x = self.fuse(torch.cat([shortcut, x], dim=-1))\n",
        "        x = self.mlp(x)\n",
        "\n",
        "        return x, mask\n"
      ],
      "metadata": {
        "id": "rXyKSFWa8cIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Parameters for the SwinTransformerBlock\n",
        "dim = 96                    # Number of input channels\n",
        "input_resolution = (28, 28) # Resolution of the input feature map (e.g., 28x28)\n",
        "num_heads = 4               # Number of attention heads\n",
        "window_size = 7             # Window size for W-MSA\n",
        "shift_size = 3              # Shift size for SW-MSA\n",
        "mlp_ratio = 4.0             # Ratio of MLP hidden dim to embedding dim\n",
        "\n",
        "# Initialize the SwinTransformerBlock\n",
        "swin_block = SwinTransformerBlock(\n",
        "    dim=dim,\n",
        "    input_resolution=input_resolution,\n",
        "    num_heads=num_heads,\n",
        "    window_size=window_size,\n",
        "    shift_size=shift_size,\n",
        "    mlp_ratio=mlp_ratio\n",
        ")\n",
        "\n",
        "# Dummy input tensor: (batch_size, H*W, dim)\n",
        "batch_size = 2\n",
        "H, W = input_resolution\n",
        "L = H * W\n",
        "x = torch.randn(batch_size, L, dim)\n",
        "\n",
        "# Dummy mask (optional): Same shape as x with only 1 channel\n",
        "mask = torch.ones(batch_size, L, 1)\n",
        "\n",
        "# Forward pass\n",
        "output, output_mask = swin_block(x, x_size=input_resolution, mask=mask)\n",
        "\n",
        "# Print output shapes\n",
        "print(\"Output shape:\", output.shape)        # Expected: (batch_size, H*W, dim)\n",
        "if output_mask is not None:\n",
        "    print(\"Output mask shape:\", output_mask.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpdyUZJu8zKp",
        "outputId": "71de5b36-069c-417e-dc13-96ea38d73d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 784, 96])\n",
            "Output mask shape: torch.Size([2, 784, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y8v2Rmb86UZ",
        "outputId": "20fde482-a537-49ee-86fa-8bbffcc4b7f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PatchMerging module"
      ],
      "metadata": {
        "id": "dwAZqPjA9SU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=2):\n",
        "        super().__init__()\n",
        "        self.conv = Conv2dLayerPartial(in_channels=in_channels,\n",
        "                                       out_channels=out_channels,\n",
        "                                       kernel_size=3,\n",
        "                                       activation='lrelu',\n",
        "                                       down=down,\n",
        "                                       )\n",
        "        self.down = down\n",
        "\n",
        "    def forward(self, x, x_size, mask=None):\n",
        "        x = token2feature(x, x_size)\n",
        "        if mask is not None:\n",
        "            mask = token2feature(mask, x_size)\n",
        "        x, mask = self.conv(x, mask)\n",
        "        if self.down != 1:\n",
        "            ratio = 1 / self.down\n",
        "            x_size = (int(x_size[0] * ratio), int(x_size[1] * ratio))\n",
        "        x = feature2token(x)\n",
        "        if mask is not None:\n",
        "            mask = feature2token(mask)\n",
        "        return x, x_size, mask\n"
      ],
      "metadata": {
        "id": "LV_U108K9Lei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def token2feature(x, x_size):\n",
        "    B, N, C = x.shape\n",
        "    h, w = x_size\n",
        "    x = x.permute(0, 2, 1).reshape(B, C, h, w)\n",
        "    return x\n",
        "\n",
        "\n",
        "def feature2token(x):\n",
        "    B, C, H, W = x.shape\n",
        "    x = x.view(B, C, -1).transpose(1, 2)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "CyeEAkKV9xjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patchupsampling"
      ],
      "metadata": {
        "id": "1U8dVO9O_gSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchUpsampling(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, up=2):\n",
        "        super().__init__()\n",
        "        self.conv = Conv2dLayerPartial(in_channels=in_channels,\n",
        "                                       out_channels=out_channels,\n",
        "                                       kernel_size=3,\n",
        "                                       activation='lrelu',\n",
        "                                       up=up,\n",
        "                                       )\n",
        "        self.up = up\n",
        "\n",
        "    def forward(self, x, x_size, mask=None):\n",
        "        x = token2feature(x, x_size)\n",
        "        if mask is not None:\n",
        "            mask = token2feature(mask, x_size)\n",
        "        x, mask = self.conv(x, mask)\n",
        "        if self.up != 1:\n",
        "            x_size = (int(x_size[0] * self.up), int(x_size[1] * self.up))\n",
        "        x = feature2token(x)\n",
        "        if mask is not None:\n",
        "            mask = feature2token(mask)\n",
        "        return x, x_size, mask\n"
      ],
      "metadata": {
        "id": "4FqVm1tw_C2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "num_patches = 784  # Assuming initial patch resolution is 28x28\n",
        "in_channels = 96\n",
        "out_channels = 192\n",
        "input_resolution = (28, 28)\n",
        "up_factor = 2\n",
        "\n",
        "# Create example input data\n",
        "x = torch.randn(batch_size, num_patches, in_channels)\n",
        "mask = torch.ones(batch_size, num_patches, 1)  # Example mask\n",
        "\n",
        "# Initialize PatchUpsampling\n",
        "patch_upsampling = PatchUpsampling(in_channels=in_channels, out_channels=out_channels, up=up_factor)\n",
        "\n",
        "# Run forward pass\n",
        "output, output_size, output_mask = patch_upsampling(x, input_resolution, mask)\n",
        "\n",
        "# Print output shapes\n",
        "print(\"Output shape:\", output.shape)         # Expected: [batch_size, new_num_patches, out_channels]\n",
        "print(\"Output resolution:\", output_size)     # Expected: (56, 56)\n",
        "print(\"Output mask shape:\", output_mask.shape if output_mask is not None else None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0v1cny4_fT7",
        "outputId": "b022cdbc-d099-4290-90f2-f38899e7cf80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 784, 192])\n",
            "Output resolution: (56, 56)\n",
            "Output mask shape: torch.Size([2, 784, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Layer"
      ],
      "metadata": {
        "id": "Dv-NzuqC_5Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicLayer(nn.Module):\n",
        "    \"\"\" A basic Swin Transformer layer for one stage.\n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        input_resolution (tuple[int]): Input resolution.\n",
        "        depth (int): Number of blocks.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        window_size (int): Local window size.\n",
        "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
        "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
        "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
        "        drop (float, optional): Dropout rate. Default: 0.0\n",
        "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
        "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
        "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
        "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, input_resolution, depth, num_heads, window_size, down_ratio=1,\n",
        "                 mlp_ratio=2., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
        "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.input_resolution = input_resolution\n",
        "        self.depth = depth\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "        # patch merging layer\n",
        "        if downsample is not None:\n",
        "            # self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
        "            self.downsample = downsample\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "        # build blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
        "                                 num_heads=num_heads, down_ratio=down_ratio, window_size=window_size,\n",
        "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
        "                                 mlp_ratio=mlp_ratio,\n",
        "                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                                 drop=drop, attn_drop=attn_drop,\n",
        "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
        "                                 norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "\n",
        "        self.conv = Conv2dLayerPartial(in_channels=dim, out_channels=dim, kernel_size=3, activation='lrelu')\n",
        "\n",
        "    def forward(self, x, x_size, mask=None):\n",
        "        if self.downsample is not None:\n",
        "            x, x_size, mask = self.downsample(x, x_size, mask)\n",
        "        identity = x\n",
        "        for blk in self.blocks:\n",
        "            if self.use_checkpoint:\n",
        "                x, mask = checkpoint.checkpoint(blk, x, x_size, mask)\n",
        "            else:\n",
        "                x, mask = blk(x, x_size, mask)\n",
        "        if mask is not None:\n",
        "            mask = token2feature(mask, x_size)\n",
        "        x, mask = self.conv(token2feature(x, x_size), mask)\n",
        "        x = feature2token(x) + identity\n",
        "        if mask is not None:\n",
        "            mask = feature2token(mask)\n",
        "        return x, x_size, mask\n"
      ],
      "metadata": {
        "id": "HJiGjvd__x4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToToken(nn.Module):\n",
        "    def __init__(self, in_channels=3, dim=128, kernel_size=5, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = Conv2dLayerPartial(in_channels=in_channels, out_channels=dim, kernel_size=kernel_size, activation='lrelu')\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x, mask = self.proj(x, mask)\n",
        "\n",
        "        return x, mask"
      ],
      "metadata": {
        "id": "84ocP1bFA6j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncFromRGB(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation):  # res = 2, ..., resolution_log2\n",
        "        super().__init__()\n",
        "        self.conv0 = Conv2dLayer(in_channels=in_channels,\n",
        "                                out_channels=out_channels,\n",
        "                                kernel_size=1,\n",
        "                                activation=activation,\n",
        "                                )\n",
        "        self.conv1 = Conv2dLayer(in_channels=out_channels,\n",
        "                                out_channels=out_channels,\n",
        "                                kernel_size=3,\n",
        "                                activation=activation,\n",
        "                                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "64vmK1bLBguF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlockDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation):  # res = 2, ..., resolution_log\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv0 = Conv2dLayer(in_channels=in_channels,\n",
        "                                 out_channels=out_channels,\n",
        "                                 kernel_size=3,\n",
        "                                 activation=activation,\n",
        "                                 down=2,\n",
        "                                 )\n",
        "        self.conv1 = Conv2dLayer(in_channels=out_channels,\n",
        "                                 out_channels=out_channels,\n",
        "                                 kernel_size=3,\n",
        "                                 activation=activation,\n",
        "                                 )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv0(x)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "EINNGNPgBkJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, res_log2, img_channels, activation, patch_size=5, channels=16, drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resolution = []\n",
        "\n",
        "        for idx, i in enumerate(range(res_log2, 3, -1)):  # from input size to 16x16\n",
        "            res = 2 ** i\n",
        "            self.resolution.append(res)\n",
        "            if i == res_log2:\n",
        "                block = EncFromRGB(img_channels * 2 + 1, nf(i), activation)\n",
        "            else:\n",
        "                block = ConvBlockDown(nf(i+1), nf(i), activation)\n",
        "            setattr(self, 'EncConv_Block_%dx%d' % (res, res), block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = {}\n",
        "        for res in self.resolution:\n",
        "            res_log2 = int(np.log2(res))\n",
        "            x = getattr(self, 'EncConv_Block_%dx%d' % (res, res))(x)\n",
        "            out[res_log2] = x\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "DrSGyg-iBnAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToStyle(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation, drop_rate):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "                Conv2dLayer(in_channels=in_channels, out_channels=in_channels, kernel_size=3, activation=activation, down=2),\n",
        "                Conv2dLayer(in_channels=in_channels, out_channels=in_channels, kernel_size=3, activation=activation, down=2),\n",
        "                Conv2dLayer(in_channels=in_channels, out_channels=in_channels, kernel_size=3, activation=activation, down=2),\n",
        "                )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = FullyConnectedLayer(in_features=in_channels,\n",
        "                                      out_features=out_channels,\n",
        "                                      activation=activation)\n",
        "        # self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.fc(x.flatten(start_dim=1))\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "NvzXKOByBvNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecBlockFirstV2(nn.Module):\n",
        "    def __init__(self, res, in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels):\n",
        "        super().__init__()\n",
        "        self.res = res\n",
        "\n",
        "        self.conv0 = Conv2dLayer(in_channels=in_channels,\n",
        "                                out_channels=in_channels,\n",
        "                                kernel_size=3,\n",
        "                                activation=activation,\n",
        "                                )\n",
        "        self.conv1 = StyleConv(in_channels=in_channels,\n",
        "                              out_channels=out_channels,\n",
        "                              style_dim=style_dim,\n",
        "                              resolution=2**res,\n",
        "                              kernel_size=3,\n",
        "                              use_noise=use_noise,\n",
        "                              activation=activation,\n",
        "                              demodulate=demodulate,\n",
        "                              )\n",
        "        self.toRGB = ToRGB(in_channels=out_channels,\n",
        "                           out_channels=img_channels,\n",
        "                           style_dim=style_dim,\n",
        "                           kernel_size=1,\n",
        "                           demodulate=False,\n",
        "                           )\n",
        "\n",
        "    def forward(self, x, ws, gs, E_features, noise_mode='random'):\n",
        "        # x = self.fc(x).view(x.shape[0], -1, 4, 4)\n",
        "        x = self.conv0(x)\n",
        "        x = x + E_features[self.res]\n",
        "        style = get_style_code(ws[:, 0], gs)\n",
        "        x = self.conv1(x, style, noise_mode=noise_mode)\n",
        "        style = get_style_code(ws[:, 1], gs)\n",
        "        img = self.toRGB(x, style, skip=None)\n",
        "\n",
        "        return x, img\n"
      ],
      "metadata": {
        "id": "fKnjXurUByrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecBlock(nn.Module):\n",
        "    def __init__(self, res, in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels):  # res = 4, ..., resolution_log2\n",
        "        super().__init__()\n",
        "        self.res = res\n",
        "\n",
        "        self.conv0 = StyleConv(in_channels=in_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               style_dim=style_dim,\n",
        "                               resolution=2**res,\n",
        "                               kernel_size=3,\n",
        "                               up=2,\n",
        "                               use_noise=use_noise,\n",
        "                               activation=activation,\n",
        "                               demodulate=demodulate,\n",
        "                               )\n",
        "        self.conv1 = StyleConv(in_channels=out_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               style_dim=style_dim,\n",
        "                               resolution=2**res,\n",
        "                               kernel_size=3,\n",
        "                               use_noise=use_noise,\n",
        "                               activation=activation,\n",
        "                               demodulate=demodulate,\n",
        "                               )\n",
        "        self.toRGB = ToRGB(in_channels=out_channels,\n",
        "                           out_channels=img_channels,\n",
        "                           style_dim=style_dim,\n",
        "                           kernel_size=1,\n",
        "                           demodulate=False,\n",
        "                           )\n",
        "\n",
        "    def forward(self, x, img, ws, gs, E_features, noise_mode='random'):\n",
        "        style = get_style_code(ws[:, self.res * 2 - 9], gs)\n",
        "        x = self.conv0(x, style, noise_mode=noise_mode)\n",
        "        x = x + E_features[self.res]\n",
        "        style = get_style_code(ws[:, self.res * 2 - 8], gs)\n",
        "        x = self.conv1(x, style, noise_mode=noise_mode)\n",
        "        style = get_style_code(ws[:, self.res * 2 - 7], gs)\n",
        "        img = self.toRGB(x, style, skip=img)\n",
        "\n",
        "        return x, img"
      ],
      "metadata": {
        "id": "lIlYWuZcB1u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, res_log2, activation, style_dim, use_noise, demodulate, img_channels):\n",
        "        super().__init__()\n",
        "        self.Dec_16x16 = DecBlockFirstV2(4, nf(4), nf(4), activation, style_dim, use_noise, demodulate, img_channels)\n",
        "        for res in range(5, res_log2 + 1):\n",
        "            setattr(self, 'Dec_%dx%d' % (2 ** res, 2 ** res),\n",
        "                    DecBlock(res, nf(res - 1), nf(res), activation, style_dim, use_noise, demodulate, img_channels))\n",
        "        self.res_log2 = res_log2\n",
        "\n",
        "    def forward(self, x, ws, gs, E_features, noise_mode='random'):\n",
        "        x, img = self.Dec_16x16(x, ws, gs, E_features, noise_mode=noise_mode)\n",
        "        for res in range(5, self.res_log2 + 1):\n",
        "            block = getattr(self, 'Dec_%dx%d' % (2 ** res, 2 ** res))\n",
        "            x, img = block(x, img, ws, gs, E_features, noise_mode=noise_mode)\n",
        "\n",
        "        return img\n",
        "\n"
      ],
      "metadata": {
        "id": "D627_ZZtB5ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecStyleBlock(nn.Module):\n",
        "    def __init__(self, res, in_channels, out_channels, activation, style_dim, use_noise, demodulate, img_channels):\n",
        "        super().__init__()\n",
        "        self.res = res\n",
        "\n",
        "        self.conv0 = StyleConv(in_channels=in_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               style_dim=style_dim,\n",
        "                               resolution=2**res,\n",
        "                               kernel_size=3,\n",
        "                               up=2,\n",
        "                               use_noise=use_noise,\n",
        "                               activation=activation,\n",
        "                               demodulate=demodulate,\n",
        "                               )\n",
        "        self.conv1 = StyleConv(in_channels=out_channels,\n",
        "                               out_channels=out_channels,\n",
        "                               style_dim=style_dim,\n",
        "                               resolution=2**res,\n",
        "                               kernel_size=3,\n",
        "                               use_noise=use_noise,\n",
        "                               activation=activation,\n",
        "                               demodulate=demodulate,\n",
        "                               )\n",
        "        self.toRGB = ToRGB(in_channels=out_channels,\n",
        "                           out_channels=img_channels,\n",
        "                           style_dim=style_dim,\n",
        "                           kernel_size=1,\n",
        "                           demodulate=False,\n",
        "                           )\n",
        "\n",
        "    def forward(self, x, img, style, skip, noise_mode='random'):\n",
        "        x = self.conv0(x, style, noise_mode=noise_mode)\n",
        "        x = x + skip\n",
        "        x = self.conv1(x, style, noise_mode=noise_mode)\n",
        "        img = self.toRGB(x, style, skip=img)\n",
        "\n",
        "        return x, img\n"
      ],
      "metadata": {
        "id": "Xga0f55hB7Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstStage(nn.Module):\n",
        "    def __init__(self, img_channels, img_resolution=256, dim=180, w_dim=512, use_noise=False, demodulate=True, activation='lrelu'):\n",
        "        super().__init__()\n",
        "        res = 64\n",
        "\n",
        "        self.conv_first = Conv2dLayerPartial(in_channels=img_channels+1, out_channels=dim, kernel_size=3, activation=activation)\n",
        "        self.enc_conv = nn.ModuleList()\n",
        "        down_time = int(np.log2(img_resolution // res))\n",
        "        for i in range(down_time):  # from input size to 64\n",
        "            self.enc_conv.append(\n",
        "                Conv2dLayerPartial(in_channels=dim, out_channels=dim, kernel_size=3, down=2, activation=activation)\n",
        "            )\n",
        "\n",
        "        # from 64 -> 16 -> 64\n",
        "        depths = [2, 3, 4, 3, 2]\n",
        "        ratios = [1, 1/2, 1/2, 2, 2]\n",
        "        num_heads = 6\n",
        "        window_sizes = [8, 16, 16, 16, 8]\n",
        "        drop_path_rate = 0.1\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
        "\n",
        "        self.tran = nn.ModuleList()\n",
        "        for i, depth in enumerate(depths):\n",
        "            res = int(res * ratios[i])\n",
        "            if ratios[i] < 1:\n",
        "                merge = PatchMerging(dim, dim, down=int(1/ratios[i]))\n",
        "            elif ratios[i] > 1:\n",
        "                merge = PatchUpsampling(dim, dim, up=ratios[i])\n",
        "            else:\n",
        "                merge = None\n",
        "            self.tran.append(\n",
        "                BasicLayer(dim=dim, input_resolution=[res, res], depth=depth, num_heads=num_heads,\n",
        "                           window_size=window_sizes[i], drop_path=dpr[sum(depths[:i]):sum(depths[:i + 1])],\n",
        "                           downsample=merge)\n",
        "            )\n",
        "\n",
        "        # global style\n",
        "        down_conv = []\n",
        "        for i in range(int(np.log2(16))):\n",
        "            down_conv.append(Conv2dLayer(in_channels=dim, out_channels=dim, kernel_size=3, down=2, activation=activation))\n",
        "        down_conv.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "        self.down_conv = nn.Sequential(*down_conv)\n",
        "        self.to_style = FullyConnectedLayer(in_features=dim, out_features=dim*2, activation=activation)\n",
        "        self.ws_style = FullyConnectedLayer(in_features=w_dim, out_features=dim, activation=activation)\n",
        "        self.to_square = FullyConnectedLayer(in_features=dim, out_features=16*16, activation=activation)\n",
        "\n",
        "        style_dim = dim * 3\n",
        "        self.dec_conv = nn.ModuleList()\n",
        "        for i in range(down_time):  # from 64 to input size\n",
        "            res = res * 2\n",
        "            self.dec_conv.append(DecStyleBlock(res, dim, dim, activation, style_dim, use_noise, demodulate, img_channels))\n",
        "\n",
        "    def forward(self, images_in, masks_in, ws, noise_mode='random'):\n",
        "        x = torch.cat([masks_in - 0.5, images_in * masks_in], dim=1)\n",
        "\n",
        "        skips = []\n",
        "        x, mask = self.conv_first(x, masks_in)  # input size\n",
        "        skips.append(x)\n",
        "        for i, block in enumerate(self.enc_conv):  # input size to 64\n",
        "            x, mask = block(x, mask)\n",
        "            if i != len(self.enc_conv) - 1:\n",
        "                skips.append(x)\n",
        "\n",
        "        x_size = x.size()[-2:]\n",
        "        x = feature2token(x)\n",
        "        mask = feature2token(mask)\n",
        "        mid = len(self.tran) // 2\n",
        "        for i, block in enumerate(self.tran):  # 64 to 16\n",
        "            if i < mid:\n",
        "                x, x_size, mask = block(x, x_size, mask)\n",
        "                skips.append(x)\n",
        "            elif i > mid:\n",
        "                x, x_size, mask = block(x, x_size, None)\n",
        "                x = x + skips[mid - i]\n",
        "            else:\n",
        "                x, x_size, mask = block(x, x_size, None)\n",
        "\n",
        "                mul_map = torch.ones_like(x) * 0.5\n",
        "                mul_map = F.dropout(mul_map, training=True)\n",
        "                ws = self.ws_style(ws[:, -1])\n",
        "                add_n = self.to_square(ws).unsqueeze(1)\n",
        "                add_n = F.interpolate(add_n, size=x.size(1), mode='linear', align_corners=False).squeeze(1).unsqueeze(-1)\n",
        "                x = x * mul_map + add_n * (1 - mul_map)\n",
        "                gs = self.to_style(self.down_conv(token2feature(x, x_size)).flatten(start_dim=1))\n",
        "                style = torch.cat([gs, ws], dim=1)\n",
        "\n",
        "        x = token2feature(x, x_size).contiguous()\n",
        "        img = None\n",
        "        for i, block in enumerate(self.dec_conv):\n",
        "            x, img = block(x, img, style, skips[len(self.dec_conv)-i-1], noise_mode=noise_mode)\n",
        "\n",
        "        # ensemble\n",
        "        img = img * (1 - masks_in) + images_in * masks_in\n",
        "\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "4y68x6HOB9r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SynthesisNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 w_dim,                     # Intermediate latent (W) dimensionality.\n",
        "                 img_resolution,            # Output image resolution.\n",
        "                 img_channels   = 3,        # Number of color channels.\n",
        "                 channel_base   = 32768,    # Overall multiplier for the number of channels.\n",
        "                 channel_decay  = 1.0,\n",
        "                 channel_max    = 512,      # Maximum number of channels in any layer.\n",
        "                 activation     = 'lrelu',  # Activation function: 'relu', 'lrelu', etc.\n",
        "                 drop_rate      = 0.5,\n",
        "                 use_noise      = True,\n",
        "                 demodulate     = True,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        resolution_log2 = int(np.log2(img_resolution))\n",
        "        assert img_resolution == 2 ** resolution_log2 and img_resolution >= 4\n",
        "\n",
        "        self.num_layers = resolution_log2 * 2 - 3 * 2\n",
        "        self.img_resolution = img_resolution\n",
        "        self.resolution_log2 = resolution_log2\n",
        "\n",
        "        # first stage\n",
        "        self.first_stage = FirstStage(img_channels, img_resolution=img_resolution, w_dim=w_dim, use_noise=False, demodulate=demodulate)\n",
        "\n",
        "        # second stage\n",
        "        self.enc = Encoder(resolution_log2, img_channels, activation, patch_size=5, channels=16)\n",
        "        self.to_square = FullyConnectedLayer(in_features=w_dim, out_features=16*16, activation=activation)\n",
        "        self.to_style = ToStyle(in_channels=nf(4), out_channels=nf(2) * 2, activation=activation, drop_rate=drop_rate)\n",
        "        style_dim = w_dim + nf(2) * 2\n",
        "        self.dec = Decoder(resolution_log2, activation, style_dim, use_noise, demodulate, img_channels)\n",
        "\n",
        "    def forward(self, images_in, masks_in, ws, noise_mode='random', return_stg1=False):\n",
        "        out_stg1 = self.first_stage(images_in, masks_in, ws, noise_mode=noise_mode)\n",
        "\n",
        "        # encoder\n",
        "        x = images_in * masks_in + out_stg1 * (1 - masks_in)\n",
        "        x = torch.cat([masks_in - 0.5, x, images_in * masks_in], dim=1)\n",
        "        E_features = self.enc(x)\n",
        "\n",
        "        fea_16 = E_features[4]\n",
        "        mul_map = torch.ones_like(fea_16) * 0.5\n",
        "        mul_map = F.dropout(mul_map, training=True)\n",
        "        add_n = self.to_square(ws[:, 0]).view(-1, 16, 16).unsqueeze(1)\n",
        "        add_n = F.interpolate(add_n, size=fea_16.size()[-2:], mode='bilinear', align_corners=False)\n",
        "        fea_16 = fea_16 * mul_map + add_n * (1 - mul_map)\n",
        "        E_features[4] = fea_16\n",
        "\n",
        "        # style\n",
        "        gs = self.to_style(fea_16)\n",
        "\n",
        "        # decoder\n",
        "        img = self.dec(fea_16, ws, gs, E_features, noise_mode=noise_mode)\n",
        "\n",
        "        # ensemble\n",
        "        img = img * (1 - masks_in) + images_in * masks_in\n",
        "\n",
        "        if not return_stg1:\n",
        "            return img\n",
        "        else:\n",
        "            return img, out_stg1\n"
      ],
      "metadata": {
        "id": "6YPGUZy0CB4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self,\n",
        "                 z_dim,                  # Input latent (Z) dimensionality, 0 = no latent.\n",
        "                 c_dim,                  # Conditioning label (C) dimensionality, 0 = no label.\n",
        "                 w_dim,                  # Intermediate latent (W) dimensionality.\n",
        "                 img_resolution,         # resolution of generated image\n",
        "                 img_channels,           # Number of input color channels.\n",
        "                 synthesis_kwargs = {},  # Arguments for SynthesisNetwork.\n",
        "                 mapping_kwargs   = {},  # Arguments for MappingNetwork.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.w_dim = w_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "\n",
        "        self.synthesis = SynthesisNet(w_dim=w_dim,\n",
        "                                      img_resolution=img_resolution,\n",
        "                                      img_channels=img_channels,\n",
        "                                      **synthesis_kwargs)\n",
        "        self.mapping = MappingNet(z_dim=z_dim,\n",
        "                                  c_dim=c_dim,\n",
        "                                  w_dim=w_dim,\n",
        "                                  num_ws=self.synthesis.num_layers,\n",
        "                                  **mapping_kwargs)\n",
        "\n",
        "    def forward(self, images_in, masks_in, z, c, truncation_psi=1, truncation_cutoff=None, skip_w_avg_update=False,\n",
        "                noise_mode='random', return_stg1=False):\n",
        "        ws = self.mapping(z, c, truncation_psi=truncation_psi, truncation_cutoff=truncation_cutoff,\n",
        "                          skip_w_avg_update=skip_w_avg_update)\n",
        "\n",
        "        if not return_stg1:\n",
        "            img = self.synthesis(images_in, masks_in, ws, noise_mode=noise_mode)\n",
        "            return img\n",
        "        else:\n",
        "            img, out_stg1 = self.synthesis(images_in, masks_in, ws, noise_mode=noise_mode, return_stg1=True)\n",
        "            return img, out_stg1\n",
        "\n"
      ],
      "metadata": {
        "id": "HbQrG_riCE_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 c_dim,                        # Conditioning label (C) dimensionality.\n",
        "                 img_resolution,               # Input resolution.\n",
        "                 img_channels,                 # Number of input color channels.\n",
        "                 channel_base       = 32768,    # Overall multiplier for the number of channels.\n",
        "                 channel_max        = 512,      # Maximum number of channels in any layer.\n",
        "                 channel_decay      = 1,\n",
        "                 cmap_dim           = None,     # Dimensionality of mapped conditioning label, None = default.\n",
        "                 activation         = 'lrelu',\n",
        "                 mbstd_group_size   = 4,        # Group size for the minibatch standard deviation layer, None = entire minibatch.\n",
        "                 mbstd_num_channels = 1,        # Number of features for the minibatch standard deviation layer, 0 = disable.\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.c_dim = c_dim\n",
        "        self.img_resolution = img_resolution\n",
        "        self.img_channels = img_channels\n",
        "\n",
        "        resolution_log2 = int(np.log2(img_resolution))\n",
        "        assert img_resolution == 2 ** resolution_log2 and img_resolution >= 4\n",
        "        self.resolution_log2 = resolution_log2\n",
        "\n",
        "        if cmap_dim == None:\n",
        "            cmap_dim = nf(2)\n",
        "        if c_dim == 0:\n",
        "            cmap_dim = 0\n",
        "        self.cmap_dim = cmap_dim\n",
        "\n",
        "        if c_dim > 0:\n",
        "            self.mapping = MappingNet(z_dim=0, c_dim=c_dim, w_dim=cmap_dim, num_ws=None, w_avg_beta=None)\n",
        "\n",
        "        Dis = [DisFromRGB(img_channels+1, nf(resolution_log2), activation)]\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            Dis.append(DisBlock(nf(res), nf(res-1), activation))\n",
        "\n",
        "        if mbstd_num_channels > 0:\n",
        "            Dis.append(MinibatchStdLayer(group_size=mbstd_group_size, num_channels=mbstd_num_channels))\n",
        "        Dis.append(Conv2dLayer(nf(2) + mbstd_num_channels, nf(2), kernel_size=3, activation=activation))\n",
        "        self.Dis = nn.Sequential(*Dis)\n",
        "\n",
        "        self.fc0 = FullyConnectedLayer(nf(2)*4**2, nf(2), activation=activation)\n",
        "        self.fc1 = FullyConnectedLayer(nf(2), 1 if cmap_dim == 0 else cmap_dim)\n",
        "\n",
        "        # for 64x64\n",
        "        Dis_stg1 = [DisFromRGB(img_channels+1, nf(resolution_log2) // 2, activation)]\n",
        "        for res in range(resolution_log2, 2, -1):\n",
        "            Dis_stg1.append(DisBlock(nf(res) // 2, nf(res - 1) // 2, activation))\n",
        "\n",
        "        if mbstd_num_channels > 0:\n",
        "            Dis_stg1.append(MinibatchStdLayer(group_size=mbstd_group_size, num_channels=mbstd_num_channels))\n",
        "        Dis_stg1.append(Conv2dLayer(nf(2) // 2 + mbstd_num_channels, nf(2) // 2, kernel_size=3, activation=activation))\n",
        "        self.Dis_stg1 = nn.Sequential(*Dis_stg1)\n",
        "\n",
        "        self.fc0_stg1 = FullyConnectedLayer(nf(2) // 2 * 4 ** 2, nf(2) // 2, activation=activation)\n",
        "        self.fc1_stg1 = FullyConnectedLayer(nf(2) // 2, 1 if cmap_dim == 0 else cmap_dim)\n",
        "\n",
        "    def forward(self, images_in, masks_in, images_stg1, c):\n",
        "        x = self.Dis(torch.cat([masks_in - 0.5, images_in], dim=1))\n",
        "        x = self.fc1(self.fc0(x.flatten(start_dim=1)))\n",
        "\n",
        "        x_stg1 = self.Dis_stg1(torch.cat([masks_in - 0.5, images_stg1], dim=1))\n",
        "        x_stg1 = self.fc1_stg1(self.fc0_stg1(x_stg1.flatten(start_dim=1)))\n",
        "\n",
        "        if self.c_dim > 0:\n",
        "            cmap = self.mapping(None, c)\n",
        "\n",
        "        if self.cmap_dim > 0:\n",
        "            x = (x * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n",
        "            x_stg1 = (x_stg1 * cmap).sum(dim=1, keepdim=True) * (1 / np.sqrt(self.cmap_dim))\n",
        "\n",
        "        return x, x_stg1\n"
      ],
      "metadata": {
        "id": "TQ156rMBCHEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8kE2iomWPFCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d0Z0vLJ3oY3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Provide the full path to your pickle file\n",
        "file_path = '/content/drive/MyDrive/Places_512.pkl'\n",
        "\n",
        "# Load the pickle file\n",
        "with open(file_path, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "\n",
        "# Use the loaded data\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "csPgltLf5g0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc94fe52-3b76-4088-ae22-b35c13e3b114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'training_set_kwargs': {'class_name': 'datasets.dataset_512_resize.ImageFolderMaskDataset', 'path': '/data/ceph/gavinqi/data/places/data_large', 'use_labels': False, 'max_size': 1803414, 'xflip': True, 'resolution': 512}, 'val_set_kwargs': {'class_name': 'datasets.dataset_512_resize.ImageFolderMaskDataset', 'path': '/data/ceph/gavinqi/data/places/validation/val_512', 'use_labels': False, 'max_size': 36500, 'xflip': False, 'resolution': 512}, 'G': Generator(\n",
            "  (synthesis): SynthesisNet(\n",
            "    (first_stage): FirstStage(\n",
            "      (conv_first): Conv2dLayerPartial(\n",
            "        (conv): Conv2dLayer()\n",
            "      )\n",
            "      (enc_conv): ModuleList(\n",
            "        (0-2): 3 x Conv2dLayerPartial(\n",
            "          (conv): Conv2dLayer()\n",
            "        )\n",
            "      )\n",
            "      (tran): ModuleList(\n",
            "        (0): BasicLayer(\n",
            "          (blocks): ModuleList(\n",
            "            (0-1): 2 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (1): BasicLayer(\n",
            "          (downsample): PatchMerging(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-2): 3 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (2): BasicLayer(\n",
            "          (downsample): PatchMerging(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-3): 4 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (3): BasicLayer(\n",
            "          (downsample): PatchUpsampling(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-2): 3 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (4): BasicLayer(\n",
            "          (downsample): PatchUpsampling(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-1): 2 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (down_conv): Sequential(\n",
            "        (0): Conv2dLayer()\n",
            "        (1): Conv2dLayer()\n",
            "        (2): Conv2dLayer()\n",
            "        (3): Conv2dLayer()\n",
            "      )\n",
            "      (to_style): FullyConnectedLayer()\n",
            "      (ws_style): FullyConnectedLayer()\n",
            "      (to_square): FullyConnectedLayer()\n",
            "      (dec_conv): ModuleList(\n",
            "        (0-2): 3 x DecStyleBlock(\n",
            "          (conv0): StyleConv(\n",
            "            (conv): ModulatedConv2d(\n",
            "              (affine): FullyConnectedLayer()\n",
            "            )\n",
            "          )\n",
            "          (conv1): StyleConv(\n",
            "            (conv): ModulatedConv2d(\n",
            "              (affine): FullyConnectedLayer()\n",
            "            )\n",
            "          )\n",
            "          (toRGB): ToRGB(\n",
            "            (conv): ModulatedConv2d(\n",
            "              (affine): FullyConnectedLayer()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (enc): Encoder(\n",
            "      (EncConv_Block_512x512): EncFromRGB(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_256x256): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_128x128): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_64x64): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_32x32): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_16x16): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "    )\n",
            "    (to_square): FullyConnectedLayer()\n",
            "    (to_style): ToStyle(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dLayer()\n",
            "        (1): Conv2dLayer()\n",
            "        (2): Conv2dLayer()\n",
            "      )\n",
            "      (pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): FullyConnectedLayer()\n",
            "    )\n",
            "    (dec): Decoder(\n",
            "      (Dec_16x16): DecBlockFirstV2(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_32x32): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_64x64): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_128x128): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_256x256): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_512x512): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mapping): MappingNet(\n",
            "    (fc0): FullyConnectedLayer()\n",
            "    (fc1): FullyConnectedLayer()\n",
            "    (fc2): FullyConnectedLayer()\n",
            "    (fc3): FullyConnectedLayer()\n",
            "    (fc4): FullyConnectedLayer()\n",
            "    (fc5): FullyConnectedLayer()\n",
            "    (fc6): FullyConnectedLayer()\n",
            "    (fc7): FullyConnectedLayer()\n",
            "  )\n",
            "), 'D': Discriminator(\n",
            "  (Dis): Sequential(\n",
            "    (0): DisFromRGB(\n",
            "      (conv): Conv2dLayer()\n",
            "    )\n",
            "    (1): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (2): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (3): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (4): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (5): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (6): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (7): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (8): MinibatchStdLayer()\n",
            "    (9): Conv2dLayer()\n",
            "  )\n",
            "  (fc0): FullyConnectedLayer()\n",
            "  (fc1): FullyConnectedLayer()\n",
            "  (Dis_stg1): Sequential(\n",
            "    (0): DisFromRGB(\n",
            "      (conv): Conv2dLayer()\n",
            "    )\n",
            "    (1): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (2): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (3): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (4): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (5): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (6): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (7): DisBlock(\n",
            "      (conv0): Conv2dLayer()\n",
            "      (conv1): Conv2dLayer()\n",
            "      (skip): Conv2dLayer()\n",
            "    )\n",
            "    (8): MinibatchStdLayer()\n",
            "    (9): Conv2dLayer()\n",
            "  )\n",
            "  (fc0_stg1): FullyConnectedLayer()\n",
            "  (fc1_stg1): FullyConnectedLayer()\n",
            "), 'G_ema': Generator(\n",
            "  (synthesis): SynthesisNet(\n",
            "    (first_stage): FirstStage(\n",
            "      (conv_first): Conv2dLayerPartial(\n",
            "        (conv): Conv2dLayer()\n",
            "      )\n",
            "      (enc_conv): ModuleList(\n",
            "        (0-2): 3 x Conv2dLayerPartial(\n",
            "          (conv): Conv2dLayer()\n",
            "        )\n",
            "      )\n",
            "      (tran): ModuleList(\n",
            "        (0): BasicLayer(\n",
            "          (blocks): ModuleList(\n",
            "            (0-1): 2 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (1): BasicLayer(\n",
            "          (downsample): PatchMerging(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-2): 3 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (2): BasicLayer(\n",
            "          (downsample): PatchMerging(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-3): 4 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (3): BasicLayer(\n",
            "          (downsample): PatchUpsampling(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-2): 3 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "        (4): BasicLayer(\n",
            "          (downsample): PatchUpsampling(\n",
            "            (conv): Conv2dLayerPartial(\n",
            "              (conv): Conv2dLayer()\n",
            "            )\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0-1): 2 x SwinTransformerBlock(\n",
            "              (attn): WindowAttention(\n",
            "                (q): FullyConnectedLayer()\n",
            "                (k): FullyConnectedLayer()\n",
            "                (v): FullyConnectedLayer()\n",
            "                (proj): FullyConnectedLayer()\n",
            "                (softmax): Softmax(dim=-1)\n",
            "              )\n",
            "              (fuse): FullyConnectedLayer()\n",
            "              (mlp): Mlp(\n",
            "                (fc1): FullyConnectedLayer()\n",
            "                (fc2): FullyConnectedLayer()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (conv): Conv2dLayerPartial(\n",
            "            (conv): Conv2dLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (down_conv): Sequential(\n",
            "        (0): Conv2dLayer()\n",
            "        (1): Conv2dLayer()\n",
            "        (2): Conv2dLayer()\n",
            "        (3): Conv2dLayer()\n",
            "      )\n",
            "      (to_style): FullyConnectedLayer()\n",
            "      (ws_style): FullyConnectedLayer()\n",
            "      (to_square): FullyConnectedLayer()\n",
            "      (dec_conv): ModuleList(\n",
            "        (0-2): 3 x DecStyleBlock(\n",
            "          (conv0): StyleConv(\n",
            "            (conv): ModulatedConv2d(\n",
            "              (affine): FullyConnectedLayer()\n",
            "            )\n",
            "          )\n",
            "          (conv1): StyleConv(\n",
            "            (conv): ModulatedConv2d(\n",
            "              (affine): FullyConnectedLayer()\n",
            "            )\n",
            "          )\n",
            "          (toRGB): ToRGB(\n",
            "            (conv): ModulatedConv2d(\n",
            "              (affine): FullyConnectedLayer()\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (enc): Encoder(\n",
            "      (EncConv_Block_512x512): EncFromRGB(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_256x256): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_128x128): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_64x64): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_32x32): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "      (EncConv_Block_16x16): ConvBlockDown(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): Conv2dLayer()\n",
            "      )\n",
            "    )\n",
            "    (to_square): FullyConnectedLayer()\n",
            "    (to_style): ToStyle(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2dLayer()\n",
            "        (1): Conv2dLayer()\n",
            "        (2): Conv2dLayer()\n",
            "      )\n",
            "      (pool): AdaptiveAvgPool2d(output_size=1)\n",
            "      (fc): FullyConnectedLayer()\n",
            "    )\n",
            "    (dec): Decoder(\n",
            "      (Dec_16x16): DecBlockFirstV2(\n",
            "        (conv0): Conv2dLayer()\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_32x32): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_64x64): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_128x128): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_256x256): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (Dec_512x512): DecBlock(\n",
            "        (conv0): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (conv1): StyleConv(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "        (toRGB): ToRGB(\n",
            "          (conv): ModulatedConv2d(\n",
            "            (affine): FullyConnectedLayer()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mapping): MappingNet(\n",
            "    (fc0): FullyConnectedLayer()\n",
            "    (fc1): FullyConnectedLayer()\n",
            "    (fc2): FullyConnectedLayer()\n",
            "    (fc3): FullyConnectedLayer()\n",
            "    (fc4): FullyConnectedLayer()\n",
            "    (fc5): FullyConnectedLayer()\n",
            "    (fc6): FullyConnectedLayer()\n",
            "    (fc7): FullyConnectedLayer()\n",
            "  )\n",
            "), 'augment_pipe': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import legacy\n",
        "\n",
        "\n",
        "def load_network(network_pkl, device):\n",
        "    \"\"\"Load the pretrained network.\"\"\"\n",
        "    print(f'Loading network from: {network_pkl}')\n",
        "    with open(network_pkl, 'rb') as f:\n",
        "        G_saved = legacy.load_network_pkl(f)['G_ema'].to(device).eval().requires_grad_(False) # type: ignore\n",
        "    return G_saved\n",
        "\n",
        "\n",
        "def read_image(image_path):\n",
        "    \"\"\"Read an image file into a NumPy array.\"\"\"\n",
        "    with open(image_path, 'rb') as f:\n",
        "        image = np.array(PIL.Image.open(f))\n",
        "    if image.ndim == 2:\n",
        "        image = image[:, :, np.newaxis]  # HW => HWC\n",
        "        image = np.repeat(image, 3, axis=2)\n",
        "    image = image.transpose(2, 0, 1)  # HWC => CHW\n",
        "    image = image[:3]  # Keep only 3 channels (RGB)\n",
        "    return image\n",
        "\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "def process_single_image(network_pkl, image_path, mask_path, outdir, resolution=512, truncation_psi=1.0, noise_mode='const'):\n",
        "    \"\"\"\n",
        "    Process a single image using the pretrained network and a provided mask.\n",
        "    Resize both the image and mask to 512x512 before processing.\n",
        "    \"\"\"\n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load network\n",
        "    G_saved = load_network(network_pkl, device)\n",
        "\n",
        "    # Create a new Generator instance with matching resolution\n",
        "    G = Generator(z_dim=512, c_dim=0, w_dim=512, img_resolution=resolution, img_channels=3).to(device).eval().requires_grad_(False)\n",
        "\n",
        "    # Copy parameters\n",
        "    for src_param, dest_param in zip(G_saved.parameters(), G.parameters()):\n",
        "        dest_param.data.copy_(src_param.data)\n",
        "\n",
        "    # Prepare input image\n",
        "    # Resize image to 512x512\n",
        "    input_image = Image.open(image_path).convert('RGB')\n",
        "    input_image = input_image.resize((256, 256), Image.Resampling.LANCZOS)\n",
        "    image = np.array(input_image).astype(np.float32)\n",
        "    image = (torch.from_numpy(image).float().to(device) / 127.5 - 1).permute(2, 0, 1).unsqueeze(0)  # [1, 3, 512, 512]\n",
        "\n",
        "    # Load and process the provided mask\n",
        "    # Resize mask to 512x512\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.0\n",
        "    mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)  # Ensure exact 512x512\n",
        "    mask = torch.from_numpy(mask).float().to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 512, 512]\n",
        "\n",
        "    # Debugging: Check tensor shapes\n",
        "    print(f\"Image shape: {image.shape}\")  # Expecting [1, 3, 512, 512]\n",
        "    print(f\"Mask shape: {mask.shape}\")  # Expecting [1, 1, 512, 512]\n",
        "\n",
        "    # Generate a random latent vector\n",
        "    z = torch.from_numpy(np.random.randn(1, G.z_dim)).to(device)\n",
        "    label = torch.zeros([1, G.c_dim], device=device)  # No labels\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            output = G(image, mask, z, label, truncation_psi=truncation_psi, noise_mode=noise_mode)\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error during generation: {e}\")\n",
        "            return\n",
        "\n",
        "        output = (output.permute(0, 2, 3, 1) * 127.5 + 127.5).round().clamp(0, 255).to(torch.uint8)\n",
        "        output = output[0].cpu().numpy()\n",
        "\n",
        "    # Save output image\n",
        "    output_image = Image.fromarray(output, 'RGB')\n",
        "    output_path = os.path.join(outdir, 'output_image.png')\n",
        "    output_image.save(output_path)\n",
        "    print(f\"Output saved at: {output_path}\")\n",
        "\n",
        "# Run the function\n",
        "# Paths\n",
        "network_pkl_path = '/content/drive/MyDrive/Places_512.pkl'  # Path to your pretrained .pkl file\n",
        "input_image_path = '/content/Places365_test_00000001.jpg'  # Path to your input image\n",
        "mask_image_path = '/content/000003.png'  # Path to your mask image\n",
        "output_directory = '/content/output'  # Directory to save the output image\n",
        "\n",
        "# Ensure the output directory exists\n",
        "import os\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Run the function\n",
        "process_single_image(network_pkl_path, input_image_path, mask_image_path, output_directory)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAiIfE7bp4Up",
        "outputId": "5bf312c6-475c-4219-d1aa-b6b5965e5852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading network from: /content/drive/MyDrive/Places_512.pkl\n",
            "Image shape: torch.Size([1, 3, 256, 256])\n",
            "Mask shape: torch.Size([1, 1, 256, 256])\n",
            "Error during generation: The size of tensor a (256) must match the size of tensor b (128) at non-singleton dimension 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/fenglinglwb/MAT.git\n",
        "%cd MAT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhFCgRnpu6G8",
        "outputId": "2a02d8f1-8848-4f3a-d974-8edaa29717a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MAT' already exists and is not an empty directory.\n",
            "/content/MAT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-LOuLUxKAB",
        "outputId": "27a5020e-158e-4798-fdf0-ea87f14f6f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.13)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.10.0.84)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.6)\n",
            "Collecting pyspng (from -r requirements.txt (line 11))\n",
            "  Downloading pyspng-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting ninja (from -r requirements.txt (line 12))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting imageio-ffmpeg==0.4.3 (from -r requirements.txt (line 13))\n",
            "  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.0.11)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (5.9.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 6)) (0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 9)) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (0.26.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 14)) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 16)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 16)) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 14)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 14)) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 14)) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 14)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm->-r requirements.txt (line 14)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm->-r requirements.txt (line 14)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm->-r requirements.txt (line 14)) (3.0.2)\n",
            "Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspng-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja, pyspng, imageio-ffmpeg\n",
            "  Attempting uninstall: imageio-ffmpeg\n",
            "    Found existing installation: imageio-ffmpeg 0.5.1\n",
            "    Uninstalling imageio-ffmpeg-0.5.1:\n",
            "      Successfully uninstalled imageio-ffmpeg-0.5.1\n",
            "Successfully installed imageio-ffmpeg-0.4.3 ninja-1.11.1.1 pyspng-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_image.py --network /content/drive/MyDrive/Places_512.pkl --dpath /content/data --mpath /content/Un --outdir /content/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CEJxZywxZ9e",
        "outputId": "8f90d170-7c43-49db-ad08-86e0d508c156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "Loading data from: /content/data\n",
            "Loading mask from: /content/Un\n",
            "Loading networks from: /content/drive/MyDrive/Places_512.pkl\n",
            "Prcessing: 31.png\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def resize_images_in_folder(folder_path, target_size=(512, 512)):\n",
        "    \"\"\"\n",
        "    Resize all images in the given folder to the target size and replace the originals.\n",
        "\n",
        "    Parameters:\n",
        "    folder_path (str): Path to the folder containing images.\n",
        "    target_size (tuple): The target size for resizing (width, height).\n",
        "    \"\"\"\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Check if it's a valid image file\n",
        "        try:\n",
        "            with Image.open(file_path) as img:\n",
        "                # Resize the image\n",
        "                img_resized = img.resize(target_size)\n",
        "\n",
        "                # Save the resized image in the same path\n",
        "                img_resized.save(file_path)\n",
        "\n",
        "                print(f\"Resized and replaced: {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping file {filename}: {e}\")\n",
        "\n",
        "# Example usage:\n",
        "folder_path = \"/content/data\"  # Replace with your folder path\n",
        "resize_images_in_folder(folder_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqCAuA7Gx9ZL",
        "outputId": "1b66b0f6-cd78-4609-882d-3f76d6fd0bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized and replaced: 1.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def apply_mask(input_image_path, mask_image_path, output_image_path):\n",
        "    # Open the input image and the mask image\n",
        "    input_image = Image.open(input_image_path).convert(\"RGBA\")  # Convert to RGBA to support transparency\n",
        "    mask_image = Image.open(mask_image_path).convert(\"L\")  # Convert mask to grayscale (L mode)\n",
        "\n",
        "    # Ensure the mask is the same size as the input image\n",
        "    mask_image = mask_image.resize(input_image.size)\n",
        "\n",
        "    # Create a new image where the mask is applied\n",
        "    input_image_np = np.array(input_image)\n",
        "    mask_image_np = np.array(mask_image)\n",
        "\n",
        "    # Apply the mask: we will keep the image where the mask is white and make the masked area transparent\n",
        "    # If the mask pixel is 0 (black), the output pixel will be transparent (0,0,0,0)\n",
        "    # If the mask pixel is 255 (white), the output pixel will retain the input image pixel\n",
        "\n",
        "    # Create an output image with transparency for masked areas\n",
        "    output_image_np = np.zeros_like(input_image_np)\n",
        "\n",
        "    # Where the mask is white, retain the input image pixel values\n",
        "    output_image_np[mask_image_np == 255] = input_image_np[mask_image_np == 255]\n",
        "\n",
        "    # Convert the numpy array back to an image\n",
        "    output_image = Image.fromarray(output_image_np, \"RGBA\")\n",
        "\n",
        "    # Save the output image\n",
        "    output_image.save(output_image_path)\n",
        "\n",
        "    # Optional: Show the output image\n",
        "    output_image.show()\n",
        "\n",
        "# Example usage:\n",
        "apply_mask('/content/drive/MyDrive/Data/9.jpg', '/content/drive/MyDrive/Masks/000020.png', '9.png')\n"
      ],
      "metadata": {
        "id": "WmIXniBwy2Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spYWH26e-iTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}